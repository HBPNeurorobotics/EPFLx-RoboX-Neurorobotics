{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# <center>Mini-project 0</center>\n",
    "## <center>Make the robot explore its environment by following Braitenberg rules</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='I'></a>\n",
    "\n",
    "</br><div style=\"text-align: justify\">In this exercise, you will use the The [Neurorobotics Platform (NRP)](http://neurorobotics.net/) to simulate a mobile robot. Your goal is to make this robot explores its environment by following the Braintenberg rules depicted in [our introduction to Neurorobotics](https://edge.edx.org/courses/course-v1:EPFLx+RoboX+2017_T3/courseware/8bcbc86b83394efe9da58ad4a1c834a1/d63c9029b12e422ba7371ed4bd6ec0df/1?activate_block_id=block-v1%3AEPFLx%2BRoboX%2B2017_T3%2Btype%40vertical%2Bblock%40e00689be644346cca350f519646d1fa5).</div>\n",
    "\n",
    "</br><div style=\"text-align: justify\">The [NRP](http://neurorobotics.net/) is a simulation platform developed in the [Human Brain Project](https://www.humanbrainproject.eu/en/). It connects [spiking neural networks](https://en.wikipedia.org/wiki/Spiking_neural_network) to virtual and real robots. This enables you to conduct embodiment experiments either locally, on your desktop machine, or our High Performance Computing clusters. All this can be done through an easy to use web interface or by means of a Python API. The NRP hence helps you build brains for bodies and bodies for brains. It also helps you simulate such models and collect data generated by your in-silico experiments. Note that the [NRP](http://neurorobotics.net/) has its [Online User Manual]( https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/index.html).</div>\n",
    "\n",
    "### The robot <a id='therobot'></a>\n",
    "You are going to control a simulated Pioneer 3DX robot equipped with [differential drive](http://wiki.ros.org/diff_drive_controller) and a [laser range finder](http://wiki.ros.org/hokuyo_node). \n",
    "Using the [ROS Python interface](http://wiki.ros.org/rospy), you can send messages of type [```geometry_msgs/Twist```](http://docs.ros.org/api/geometry_msgs/html/msg/Twist.html) to the robot. These messages let you specify a linear velocity along the `x`-axis together with an angular velocity around the `z`-axis. Velocities on other components are ignored. The details of the controller implementation will be covered in a subsequent section.\n",
    "\n",
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/pioneer3dx.png)\n",
    "\n",
    "The [laser range finder](http://wiki.ros.org/hokuyo_node) has 360 beams indexed as in the picture below. For each laser beam, the device collects a value. This value a floatting point number named `range`, which corresponds to the distance in meter to the detected object. The exploration behaviour that you will implement is based on these `range` values.\n",
    "\n",
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/beams.png)\n",
    "\n",
    "### The brain <a id='thebrain'></a>\n",
    "The brain model that will be used to implement the Braitenberg exploration behaviour is depicted below. The model has four neurons and four synapses. It is defined by means of a [PyNN script](https://neuralensemble.org/PyNN/)  and it will be simulated with the [NEST simulator](https://www.nest-simulator.org/).\n",
    "\n",
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/brain.png)\n",
    "\n",
    "In your implementation of the exploration behaviour, you will bind the spiking rates of the two **sensors** neurons to the `range` values of the [laser range finder](http://wiki.ros.org/hokuyo_node). The resulting voltages of the two **actors** neurons will be turned into velocity commands.\n",
    "\n",
    "### Connecting the robot and the brain <a id='connecting'></a>\n",
    " A [Neurorobotics experiment](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/user_interface/introduction.html) in the sense of the NRP is a folder that contains\n",
    " - [configuration files](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/simulation_setup/introduction.html) made of references to environment, robot and brain models, \n",
    " - the Python files responsible for the transfer of information between the body and the brain. The latter are called **[Transfer Functions](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/simulation_setup/transfer_functions.html)**.\n",
    " \n",
    "The connexions between the Pioneer 3DX robot and the brain model are established in the transfer functions of the Neurorobotics experiment named *Mini-project0*. You will start [Section A](#A) below by making a copy of this folder in your private NRP storage. The transfer function named `laser_sensors_transmit.py` takes the [laser range finder](http://wiki.ros.org/hokuyo_node) `range` values as input and turns them into spiking rates for the two **sensors** neurons, namely `right_sensor` and `left_sensor`. The transfer function named `velocity_commands.py` takes the voltages of the **motors** neurons as input and turn them into velocity commands for the [differential drive](http://wiki.ros.org/diff_drive_controller) of the Pioneer 3DX robot. You will tweak parameters in these two files when going through [Section A.2](#A2).\n",
    "\n",
    "### Output <a id='connecting'></a>\n",
    "The main output of ***Mini-project 0*** is the file `robot_positions.csv` which contains the positons of the robot after a successful exploration of its environment. This file is required to continue with ***Mini-project 1*** and ***Mini-project 2***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "source": [
    "--------------------------------------------\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "### Content:\n",
    "- <font size=\"4\">[A. Fill in the gaps of the Braitenberg controller [NRP platform]](#A)</font>\n",
    "- <font size=\"4\">[B. Launch the neurorobotics simulation of Exercise 1 with the Virtual Coach](#B)</font>\n",
    "- <font size=\"4\">[C. Record the robot positions](#C)</font>\n",
    "- <font size=\"4\">[D. Visualize the recorded robot positions](#D)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "--------------------------------------------\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "## A. Fill in the gaps of the Braitenberg controller [NRP platform] <a id='A'></a>\n",
    "The challenges of Section A shall be completed in the simulation environment of the [NRP](http://neurorobotics.net/). When joining the [NRP Web Cockpit](http://148.187.97.48/#/esv-private), you should have view similar to the one displayed below. We will explain now how to make a copy of **Mini-Project 0** in your private NRP storage and how to edit the experiment files in order to achieve the desired robot behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/nrp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### A.1. Copy ***Mini-project 0***'s experiment template into your private storage\n",
    "\n",
    " You need first to take ownership of the template called ***\"Mini-project 0: Make the robot explore ...\"*** by making a \n",
    " copy of it into your NRP private storage. This is a 3-step process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "- Join the [NRP Web Cockpit](http://148.187.96.224/#/esv-private?dev) and select the ***Templates*** tab. Your browser view is depicted below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/templates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "source": [
    "- Then select the experiment whose title starts with ***\"Exercise 1: Make the robot explore ...\"***. Note that you can use the filter at the top right-end corner.\n",
    "- Finally, press the ***Clone*** button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/clone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "source": [
    "Now you own a copy of the experiment which is visible in ***My Experiments*** tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/my-experiments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### A.2. Tune actuator and sensor parameters in order to achieve exploration and obstacle avoidance <a id='A2'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "source": [
    "Select the experiment named ***\"Exercise 1: Make the robot explore ...\"*** in ***My Experiments*** tab and press the launch button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/launch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    " Open then the [*Transfer Function Editor*](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/user_interface/edit/7-gz3d-tf-editor.html) by pressing the following button:\n",
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/tf-icon.png)\n",
    "\n",
    "Then edit the files *velocity_commands.py* and *laser_sensors_transmit.py*. Follow the comments left in the code and tweak the values of the parameters `lin_max`, `lin_factor` and `ang_factor` in the first file, and the values of `idx_right`, `x_0` and `y_0` in the second file until the robot starts exploring its environment while avoiding obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/tf-editor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "source": [
    "__A.2.1 Hints for *velocity_commands.py*__\n",
    "\n",
    "Try successively these different values: (0.1, 0.2, 0.6) for `lin_max`, (80, 100, 120) for `lin_factor` and (30, 60, 90) for `ang_factor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": false
   },
   "source": [
    "__A.2.2 Hints for *laser_sensors_transmit.py*__\n",
    "\n",
    "- Depict for yourself the graph of the function `activation_fct(x, x_intercept, y_intercept)`.\n",
    "- Try successively these different values: (1, 2, 3) for `x_0`, (70, 90, 100) for `y_0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "__A.2.3 The Log console as debug tool__\n",
    "\n",
    "Press the **Log Console** button to see the logs printed by commands of the type `clientLogger.info('some useful information'))`:\n",
    "\n",
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/log-console-icon.png)\n",
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/log-console.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### A.3. Move automatically the robot when it gets stuck\n",
    "Open the [SMACH Script Editor](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/user_interface/edit/7-gz3d-edit-simulation.html) by pressing on the following button:\n",
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/smach-editor-icon.png)\n",
    "![title](https://raw.githubusercontent.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics/master/Neurorobotics_MOOC_initialization/notebooks/images/smach-editor.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "source": [
    "Edit the file *move_robot.exd* so that the robot is moved every time it gets stuck. You need to add a new state of type `SetModelPose` to the state machine. This state is returned by the function `new_spawn()` provided for your convenience.The [SMACH StateMachines documentation](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/tutorials/experiment/state_machines.html?highlight=state%20machine) provides you with the API and few examples.\n",
    "\n",
    "<font color=red>__Important:__</font> Save your changes and stop the simulation from the graphical user interface before going to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "source": [
    "--------------------------------------------\n",
    "\n",
    "--------------------------------------------\n",
    "## B. Launch a neurorobotics simulation with the Virtual Coach (dry run)<a id='B'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The goal of this section is that you get familiar with the [Virtual Coach](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/virtual_coach/introduction.html), a means of controlling simulations by scripts instead of using the Web Cockpit. You will only perform a **dry run**: start and then stop a single simulation through few python commands.\n",
    "\n",
    "You need first to install the [Virtual Coach](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/virtual_coach/introduction.html), the Python API that allows you to run and interact with the experiments of the [Neurorobotics platform](http://148.187.97.48/#/esv-private)\n",
    "<font color=red>__Important:__</font> Run the cell below everytime you re-open this Jupyter notebook and go over this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B.1.__ Install NRP's [Virtual Coach](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/virtual_coach/introduction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# Install NRP's Virtual Coach\n",
    "!pip install -i https://bbpteam.epfl.ch/repository/devpi/simple  hbp-nrp-virtual-coach==2.2.3\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "--------------------------------------\n",
    "__B.2.__ Initialize NRP's  [Virtual Coach](https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/2.0/nrp/user_manual/virtual_coach/introduction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hbp_nrp_virtual_coach.virtual_coach import VirtualCoach\n",
    "print(\"Retrieving your HBP OIDC token\")\n",
    "token = str(oauth.get_token())\n",
    "print(\"token retrieved!\")\n",
    "vc = VirtualCoach(environment='dev', oidc_token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "--------------------------------------\n",
    "__B.3.__ The next command will display the list of experiments located in your NRP storage (not to be confused with this Collab  <span style=\"background-color: #f6d351\">Storage</span>). <font color=red>\n",
    "    \n",
    "__Important:__</font> check that __*'Exercise1_0'*__, possibly __*'Exercise1_<font color=red>i</font>'*__ if you made *i+1* copies, is one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vc.print_cloned_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "source": [
    "--------------------------------------\n",
    "__B.4.__ Launch <font color=black>__*Exercise 1_i*__</font> 's  experiment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "style = {'description_width': 'initial'}\n",
    "experiment_widget = widgets.Text(\n",
    "    description='Experiment name', \n",
    "    placeholder='Copy here the experiment name taken from the above list',\n",
    "    style=style,\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "display(experiment_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = vc.launch_experiment(str(experiment_widget.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can go to the [Neurorobotics Platform](http://148.187.97.48/#/esv-private), check the ***Running Simulations*** tab and join your simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "--------------------------------------\n",
    "__B.5.__ Stop the simulation\n",
    "\n",
    "You have reached the end of this dry run. Stop the simulation with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "source": [
    "--------------------------------------------\n",
    "\n",
    "--------------------------------------------\n",
    "## C. Record the robot positions <a id='C'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "__C.1.__ ***From now on, we assume that you correctly implemented the Braitenberg exploration behaviour of the robot.***\n",
    "\n",
    "You are going to launch __*Exercise 1*__ 's experiment again [(see Section B)](#B) and let the simulation record the robot positions for a sufficiently long time.\n",
    "\n",
    "To do so, you will have to:\n",
    "\n",
    "- input the name of __*Exercise 1_i*__ according to a list of experiments that you will print below, \n",
    "- input __simulation time__ in seconds. A sufficiently long time is not less than __900 seconds__.\n",
    "\n",
    "Once you have run an experiment, you can join the simulation via the [Neurorobotics Platform](http://148.187.96.224/#/esv-private?dev) and check if the robot does a proper job. In the mean time, a transfer function is recording the robot position every 20 ms into the file `robot_positions.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "You need to install the **[HBP Service Client](https://collab.humanbrainproject.eu/#/collab/509/nav/24072)**.\n",
    "This is a Python API that provides convenient access to Collaboratory's Storage service. We will use it to save the generated file `robot_positions.csv` to this Collab storage.\n",
    "\n",
    "<font color=red>__Important:__</font> Run the cell below everytime you re-open this Jupyter notebook and go over this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# Install HBP's ServiceClient\n",
    "!pip install --upgrade \"hbp-service-client==1.1.1\"\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the Virtual Coach:\n",
    "from hbp_nrp_virtual_coach.virtual_coach import VirtualCoach\n",
    "\n",
    "print(\"Retrieving your HBP OIDC token\")\n",
    "token = str(oauth.get_token())\n",
    "print(\"token retrieved!\")\n",
    "\n",
    "# Fill in your password when prompted\n",
    "vc = VirtualCoach(environment='dev', oidc_token=token)\n",
    "vc.print_cloned_experiments()\n",
    "\n",
    "# Launch Exercise 1's experiment for a sufficiently long time\n",
    "experiment = raw_input('Experiment name: ')\n",
    "sim_time = input('Simulation time (s): ')\n",
    "\n",
    "# Display the list of experiments located in your storage\n",
    "sim = vc.launch_experiment(experiment)\n",
    "\n",
    "# Run the experiment\n",
    "sim.start()\n",
    "\n",
    "# Wait for the specified duration. At this moment, you can check the simulation process by joining NRP platform.\n",
    "import time; time.sleep(sim_time); sim.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": false
   },
   "source": [
    "__C.2.__ Retrieve the latest recorded robot positions from the NRP storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the latest recorded positions\n",
    "filename = 'robot_positions.csv'\n",
    "csv_content = vc.get_last_run_csv_file(experiment, filename)\n",
    "# Write robot positions into file \n",
    "wr = open(filename, 'w')\n",
    "wr.write(csv_content)\n",
    "\n",
    "# Get service client information\n",
    "clients = get_hbp_service_client()\n",
    "collab_path = get_collab_storage_path()\n",
    "\n",
    "# Remove the previous recorded positions, if needed\n",
    "from os import path\n",
    "filepath = path.join(collab_path, filename)\n",
    "if clients.storage.exists(filepath): \n",
    "    clients.storage.delete(filepath)\n",
    "    \n",
    "# Upload the latest recorded robot positions to your Collab storage\n",
    "data = clients.storage.upload_file(filename, filepath, 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": false
   },
   "source": [
    "__C.3.__ Stop the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "sim.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": false
   },
   "source": [
    "--------------------------------------------\n",
    "\n",
    "--------------------------------------------\n",
    "## D. Visualize the recorded robot positions <a id='D'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": false
   },
   "source": [
    "In this section, you only need to execute each cell in order to visualize the recorded robot positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "scrolled": false
   },
   "source": [
    "**D.1.** Transform the latest recorded positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from StringIO import StringIO\n",
    "from os import path\n",
    "filename = 'robot_positions.csv'\n",
    "\n",
    "# Download the latest recorded positions from your Collab storage\n",
    "clients = get_hbp_service_client()\n",
    "collab_path = get_collab_storage_path()\n",
    "filepath = path.join(collab_path, filename)\n",
    "clients.storage.download_file(filepath, path.join('.', filename))\n",
    "\n",
    "# Process data for visualization\n",
    "data = pd.read_csv(filename, delimiter=',',header=0).values\n",
    "positions = np.array([pd.to_numeric(data[:,0], errors='coerce'), pd.to_numeric(data[:,1], errors='coerce')]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "**D.2.** Visualization of recorded robot positions as exploration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "# Figure\n",
    "fig = plt.figure(0,figsize=(8, 6))\n",
    "plt.title('Exploration data collected by the robot')\n",
    "\n",
    "# Obstacles\n",
    "ax = fig.add_subplot(111)\n",
    "rect1 = patches.Rectangle((-1.0,-3.0), 2., 1., color='black'); ax.add_patch(rect1)\n",
    "rect2 = patches.Rectangle(( 1.0,-3.0), 1., 3., color='black'); ax.add_patch(rect2)\n",
    "rect3 = patches.Rectangle((-2.0, 0.0), 1., 2., color='black'); ax.add_patch(rect3)\n",
    "rect4 = patches.Rectangle((-2.0, 2.0), 3., 1., color='black'); ax.add_patch(rect4)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(positions[:,1], positions[:,0], s=4e-3, color='r')\n",
    "plt.axis([-4.8, 4.8, -4.8, 4.8])\n",
    "plt.gca().invert_yaxis()\n",
    "ax.axes.get_xaxis().set_visible(False)\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
