{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "# Navigate the robot between any two points of its environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "\n",
    "--------------------------------------\n",
    "### Initialization. Get data from solved Excercise 1\n",
    "\n",
    "*Explain how to go over from Excercise 2 to Excercise 3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "### Content:\n",
    "- <font size=\"4\">**[A. Utils installation and downloads](#A)**</font></br>\n",
    ">- <font size=\"3\">[A.1. Install SOM and SARSA utils](#A1)</font></br>\n",
    ">- <font size=\"3\">[A.2. Download robot positions and SOM lattice from your Collab Storage](#A2)</font></br>\n",
    "- <font size=\"4\">**[B. Perform SOM training (editable) ](#B)**</font></br>\n",
    "- <font size=\"4\">**[C. Complete the SARSA implementation](#C)**</font></br>\n",
    ">- <font size=\"3\">[C.1. SARSA description](#C1)</font></br>\n",
    ">- <font size=\"3\">[C.2. SARSA function (TO DO)](#C2)</font></br>\n",
    ">- <font size=\"3\">[C.3. Upload SARSA function to your Collab storage](#C3)</font></br>\n",
    "- <font size=\"4\">**[D. SARSA simulation ](#D)**</font></br>\n",
    ">- <font size=\"3\">[D.1. Perform SARSA training (editable) ](#D1)</font></br>\n",
    ">- <font size=\"3\">[D.2. Save SOM training result ](#D2)</font></br>\n",
    "- <font size=\"4\">**[E. Result evaluation](#E)**</font></br>\n",
    "- <font size=\"4\">**[F. Robot navigation within NRP platform (SOM & SARSA application)](#F)**</font></br>\n",
    ">- <font size=\"3\">[F.1. Generate test for NRP platform ](#F1)</font></br>\n",
    ">- <font size=\"3\">[F.2. Save NRP test ](#F2)</font></br>\n",
    ">- <font size=\"3\">[F.3. Run experiment within NRP platform](#F3)</font></br>\n",
    "- <font size=\"4\">**[G. Submission and grading ](#G)**</font></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------\n",
    "\n",
    "---------------------------------------------------\n",
    "## A. Utils installation and download of data from Collab storage<a id='A'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1. Install the SARSA and SOM utils <a id='A1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "! pip uninstall epflx_robox_nrp_utils -y\n",
    "! pip install --user --force-reinstall git+https://github.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics-utils#egg=epflx_robox_nrp_utils\n",
    "! pip install --upgrade \"hbp-service-client==1.1.1\"\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2. Download robot positions and SOM lattice from your Collab Storage <a id='A2'></a>\n",
    "Run the following cell to download the robot positions and the SOM lattice (csv files) from your Collab Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = get_hbp_service_client()\n",
    "collab_path = get_collab_storage_path()\n",
    "from os import path\n",
    "\n",
    "filename = raw_input(\"(Robot positions) Enter the name of the data file you want to use for simulation:\")\n",
    "if not filename: \n",
    "    filename = 'robot_positions.csv'; clear_output()\n",
    "filepath = path.join(collab_path, filename)\n",
    "clients.storage.download_file(filepath, './robot_positions.csv')\n",
    "\n",
    "filename = raw_input(\"(SOM lattice) Enter the name of the data file you want to use for simulation:\")\n",
    "if not filename: \n",
    "    filename = 'SOM_data_lattice.csv'; clear_output()\n",
    "filepath = path.join(collab_path, filename)\n",
    "clients.storage.download_file(filepath, './SOM_data_lattice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function used to save files to your Collab storage\n",
    "\n",
    "\"\"\"\n",
    "Save a file to the Collab storage\n",
    "\n",
    ":param filepath: relative path to the file\n",
    ":param filetype: type of the file, e.g., 'text/csv' or 'text/x-python'\n",
    ":param remove: remove the file of the Jupyter notebook space if True \n",
    "\"\"\"\n",
    "def save_to_collab_storage(filename, filetype, remove=False):\n",
    "    from os import path\n",
    "    clients = get_hbp_service_client()\n",
    "    collab_path = get_collab_storage_path()\n",
    "    filename = path.basename(filepath)\n",
    "    # if you create new file\n",
    "    if os.path.isfile(filename):\n",
    "        # if old version of the file exists\n",
    "        filepath = os.path.join(collab_name, filename)\n",
    "        if clients.storage.exists(filepath): \n",
    "            clients.storage.delete(filepath)\n",
    "        pydata = clients.storage.upload_file(filename, filepath, filetype)\n",
    "        # remove \n",
    "        if remove:\n",
    "            os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "\n",
    "-----------------------------------------------------\n",
    "## B. SARSA data preparation: generate your maze based on the SOM data <a id='B'></a>\n",
    "In Exercise 2, we obtained the SOM-lattice that basically plays the role of reducing the data. \n",
    "At this stage, we transition from the space of Cartesian coordinates of the robot environment to the SOM-lattice coordinates. This means that the robot will move straigth from one point of the SOM-lattice to another one. \n",
    "But not all of the points are available for transitioning. So, here we define which directions are available and we generate a reward matrix based on this information. You will go through the following steps:\n",
    "\n",
    "- You can see 2 images wich represents the translation from Cartesian coordinates to SOM-lattice coordinates.\n",
    "- Input the goal position that you want to use as [y,x] with respect to SOM-lattice coordinates.\n",
    "- Analyse the Reward matrix you obtain. This matrix is of the form [N,N,4], where **N** is size of the SOM-lattice and **4** is number of possible actions (Down, Up, Right, Left). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from epflx_robox_nrp_utils.SARSA.SARSA_additional import SARSA_additional\n",
    "sarsaad = SARSA_additional()\n",
    "reward, actions = sarsaad.run_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to save your result into your Collab storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_collab_storage(filepath='SOM_possible_actions.csv', filetype='text/csv', remove=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "\n",
    "-----------------------------------------------------\n",
    "## C. SARSA data preparation: generate your maze based on the SOM data <a id='C'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1. SARSA description <a id='C1'></a>\n",
    "****\n",
    "\n",
    "**Exercise goal:**<br>\n",
    "Your task is to implement a State-Action-Reward-State-Action algorithm (SARSA) to navigate the agent environment.\n",
    "\n",
    "**Input:** SOM-lattice given by Exercise 2 (*'SOM_data_lattice.csv'*).<br>\n",
    "In order to run the <font color=green>**_SARSA data preparation_**</font> cell, you have to generate a maze and a reward table ('SARSA_data_reward.csv') based on the SOM lattice data.<br>\n",
    "\n",
    "**Output:** Expected reward matrix [Q-values] (*'SARSA_data_Qvalue.csv'*).<br>\n",
    "When running the <font color=green>**_NRP platform test_**</font> cell, you can generate a random start position , or enter yours, and a list of way points (*'SARSA_data_way_points.csv'*) to reach a goal based on your SARSA learning.<br>\n",
    "****\n",
    "\n",
    "**SARSA algorithm:**<br>\n",
    "*<font color=green>**while**</font> (N_trials are not done):*<br>\n",
    "> <font color=blue>*# TRIAL: run_trial()*</font><br>\n",
    "> *initialize eligibility*<br>\n",
    "> *update policy (greedy epsilon)*<br>\n",
    "> *generate random start position (take care the availability of that position)*<br>\n",
    "> *make a choice of action: choose_action()*<br>\n",
    "> *<font color=green>**while**</font> (agent is not in goal):*<br>\n",
    ">> *<font color=blue># EPISODE: run_episode():</font>*<br>\n",
    ">> *change current agent position: update_state()*<br>\n",
    ">> *make a choice of action: choose_action()*<br>\n",
    ">> *update the eligibility trace: update_E()*<br>\n",
    ">> *update the expected reward: update_Q()*<br>\n",
    ">> *update the latency parameter*<br>\n",
    ">> *<font color=blue># ----------------------------------</font>*<br>\n",
    ">> *checkup of trial end: agent in the goal (also, lattency limit can make trial end)*<br>\n",
    "\n",
    "****\n",
    "\n",
    "\n",
    "**Initialization functions:**<br>\n",
    " - **`init_parameters()`:** generate new order of robot positions; <font color=green>**return**</font> *__array__ of new order*.<br>\n",
    " - **`init_trial()`:** generate new order of robot positions; <font color=green>**return**</font> *__array__ of new order*.<br>\n",
    " \n",
    "**Update functions:**<br>\n",
    " - **`update_state()`:** Remember the old position of the agent before moving on. Update the agents position according to the chosen action. Check if the agent has bumped into a wall (move to is_wall).; <font color=green>**return**</font> *__nothing__*.<br>\n",
    " - **`update_E()`:** Update the eligibility trace according to SARSA; <font color=green>**return**</font> *__nothing__*.<br>\n",
    " - **`update_Q()`:** Update the Q-values according to SARSA; <font color=green>**return**</font> *__nothing__*.<br>\n",
    " \n",
    "**Interaction functions:**<br>\n",
    " - **`choose_action()`:** Choose the next action based on the current estimate of the Q-values. The parameter epsilon determines how often the agent chooses the action with the highest Q-value (probability `1 - epsilon`). In the rest of the cases, a random action is chosen; <font color=green>**return**</font> *__nothing__*.<br>\n",
    " - **`reward()`:** Evaluate which reward should be given when performing the chosen action at the current location; <font color=green>**return**</font> *__reward__ value for chosen action*.<br>\n",
    "\n",
    "**Checkup functions:**<br>\n",
    " - **`is_trial_end()`:** checkup if it is the end trial; <font color=green>**return**</font> *boolean value*.<br>\n",
    " - **`is_bump()`:** checks if the agent bumps into a wall; <font color=green>**return**</font> *boolean value*.<br>\n",
    " - **`is_wall()`:** checks if the agent sets up on the wall; <font color=green>**return**</font> *boolean value*.<br>\n",
    "****\n",
    "\n",
    "\n",
    "**Global variables initialization (<font color=blue>default</font>, <font color=red>yours</font>):**<br>\n",
    " - <font color=blue>**self.video:**</font>visualization trigger (0 - no; 1 - mode1; 2 - mode2; 3 - latency).<br>\n",
    " - <font color=blue>**self.pos:**</font>data from input file ( <font color=red>*'NRP_data_robot_positions.csv'*</font> ).<br>\n",
    " - <font color=blue>**self.lattice:**</font> the initial randomly generated SOM.<br>\n",
    " - <font color=blue>**self.Nn:**</font>maze size, equal to SOM size [_Nn_ x _Nn_].<br>\n",
    " <br>\n",
    " - <font color=blue>**self.reward_position:**</font> the goal position. You can enter yours when running the <font color=green>**_SARSA data preparation_**</font> cell, otherwise the goal position will be generated randomly.<br>\n",
    " - <font color=blue>**self.Reward:**</font> the reward table taken from input file ( <font color=red>*'SARSA_data_reward.csv'*</font> ) that is generated by running of the <font color=green>**_SARSA data preparation_**</font> cell.<br>\n",
    " <br>\n",
    " - <font color=blue>**self.latency:**</font>latency of the current trial (initial value is 0).<br>\n",
    " - <font color=blue>**self.latency_list:**</font> is **empty list** by default. This list will accumulate latency of each trial.<br>\n",
    " - <font color=blue>**self.trial:**</font>index of the current trial (initial value is 0).<br>\n",
    "<br>\n",
    " - <font color=blue>**[self.x_position, self.y_position]:**</font> current position of the agent (in terms of SARSA algorithm - **_S'_**).<br>\n",
    " - <font color=blue>**[self.x_position_old, self.y_position_old]:**</font> the previous position of the agent (in terms of SARSA algorithm - **_S_**).<br>\n",
    " - <font color=blue>**[self.x_start, self.y_start]:**</font>random position of the agent at the very begining of the trial.<br>\n",
    " - <font color=blue>**self.action:**</font>current chosen action of the agent (in terms of SARSA algorithm - **_A'_**).<br>\n",
    " - <font color=blue>**self.action_old:**</font>previous chosen action of the agent (in terms of SARSA algorithm - **_A_**).<br>\n",
    "<br>\n",
    " - <font color=red>**self.N_trials:**</font>number of trials during learning.<br>\n",
    " - <font color=red>**self.Q:**</font>initial state of the expected reward matrix [*__Nn__* x *__Nn__* x __4__] (represents the 4 possible actions: **up, down, right, left**). <br>\n",
    " - <font color=red>**self.e:**</font> the initial state of the eligibility trace matrix [*__Nn__* x *__Nn__* x __4__].<br>\n",
    " <br>\n",
    " - <font color=red>**self.reward_at_target:**</font>reward given to the agent when reaching the goal.<br>\n",
    " - <font color=red>**self.reward_at_wall:**</font>reward (punishment) given to the agent when bumping into a wall.<br>\n",
    "<br>\n",
    " - <font color=red>**self.epsilon:**</font>initial probability with which the agent chooses a random action. This makes sure the agent explores the maze (**Policy: epsilon greedy**).<br>\n",
    " - <font color=red>**self.tau:**</font>time constant of SARSA algorithm.<br>\n",
    " - <font color=red>**self.eta:**</font>learning rate of SARSA algorithm.<br>\n",
    " - <font color=red>**self.gamma:**</font>discount factor - quantifies how far into the future a reward is still considered important for the current action.<br>\n",
    " - <font color=red>**self.lambda_eligibility:**</font>decay factor for the eligibility trace; the default is 0., which corresponds to no eligibility trace at all.<br>\n",
    "****\n",
    "\n",
    "<font color=red>**TO DO:**</font><br>\n",
    "**1)** Run the <font color=green>**_SARSA data preparation_**</font> cell. You can see the representations (mode1, mode2) of the maze generated based on your SOM-lattice solution. Please, enter the goal coordinates to get the reward table. Then, press enter to finish this stage.<br>\n",
    "**2)** Fill the hole (<font color=red>**episode**</font>) in SARSA algorithm **`run_trial()`**.<br>\n",
    "**3)** Fill <font color=red>Initialization functions</font> parameters <font color=red>(yours)</font> for the very begining learning **`init_parameters()`** and needed parameters for the start of each trial **`init_trial()`**.<br>\n",
    "**4)** Implement the <font color=red>Update functions</font>: agent position, eligibility trace and expected reward.<br>\n",
    "**5)** Implement the <font color=red>Interaction functions</font>: choose an action (baced on the policy) and get reward for this action.<br>\n",
    "**6)** Implement the <font color=red>Checkup functions</font>: make the agent check itself [is_trial_end(); is_bump(); is_wall()]<br>\n",
    "**7)** Run the <font color=green>**_NRP platform test_**</font> cell to generate a random start position (or enter yours) and way points (*'SARSA_data_way_points.csv'*) to reach the given goal based on your SARSA learning.<br>\n",
    "**6)** Run <font color=green>**_NRP platform_**</font> cell, join the Neurorobotics platform and clone the template named *Exercise 3: Navigate the robot ...*. Select the *Experiment files* tab and upload the two csv files *'SOM_data_lattice.csv'*, *'SARSA_data_way_points.csv'* to your NRP Storage. Then, **run the experiment to see the result**. <br>\n",
    "****\n",
    "\n",
    "<font color=red>**IMPORTANT:**</font><br>\n",
    "**1)** Please, keep in mind that the grading process has **a time limit of 60 seconds**.<br>\n",
    "**2)** Once you are ready to submit, please, uncomment the first line of your script (`%%writefile SARSA_solution.py`) and run the cell that contains the script. This way your script will be saved as a file with the given name. **Then, submit this file.**\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2. SARSA function (TO DO) <a id='C2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile SARSA_solution.py \n",
    "\n",
    "# class SARSA (Sarsa_solution)                                                                                               \n",
    "import numpy\n",
    "import time\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SARSA:\n",
    "\n",
    "\t#########################################\n",
    "\t###        Main stages of SARSA       ###\n",
    "\t#########################################\n",
    "    \n",
    "\tdef __init__(self, video=0):\n",
    "\t\t# upload lattice information\n",
    "\t\tfrom epflx_robox_nrp_utils.SARSA.SARSA_additional import SARSA_additional\n",
    "\t\tself.sarsaad = SARSA_additional()\n",
    "\n",
    "\t\tself.video = video\n",
    "\t\tself.pos = self.sarsaad.upload_positions()\n",
    "\t\tself.lattice = self.sarsaad.upload_lattice()\n",
    "\t\tself.Nn = len(self.lattice[0])\n",
    "        \n",
    "\t\t# initialize parameters\n",
    "\t\tself.init_parameters()\n",
    "        \n",
    "\t\t# reward administered at a target location and when bumping into walls\n",
    "\t\tself.Actions, self.reward_position = self.sarsaad.upload_reward()#(self)Reward\n",
    "        \n",
    "\t\t# list containing the times it took the agent to reach the target for all trials\n",
    "\t\t# serves to track the progress of learning\n",
    "\t\tself.latency_list = []\n",
    "\n",
    "\t\t# initialize the state and action variables\n",
    "\t\tself.x_position = None\n",
    "\t\tself.y_position = None\n",
    "\t\tself.action = None\n",
    "\t\tself.x_position_old = None\n",
    "\t\tself.y_position_old = None\n",
    "\t\tself.action_old = None\n",
    "        \n",
    "\n",
    "\tdef run_sarsa(self):\n",
    "\t\tT = time.time()\n",
    "\t\tfor self.trial in range(self.N_trials):\n",
    "\t\t\t# run a trial and store the time it takes to the target\n",
    "\t\t\tself.latency = self.run_trial()\n",
    "            \n",
    "\t\t\t# Add latency of last trial and visualize latency \n",
    "\t\t\tif (self.trial % 1000 == 0 or self.trial == self.N_trials): self.sarsaad.save_Qvalue(self.Q)\n",
    "\t\t\tself.latency_list.append(self.latency)\n",
    "\t\t\tif self.video == 3: self.visualization()\n",
    "                \n",
    "\t\tself.sarsaad.print_Qvalue(self.Q, self.reward_position,self.Actions)\n",
    "\t\tprint 'Done. Simulation time is ', time.time() - T, '(s).'\n",
    "\n",
    "        \n",
    "\t############################################\n",
    "\t###        Visualization of SARSA        ###\n",
    "\t############################################\n",
    "    \n",
    "\tdef visualization(self):\n",
    "\t\t# visualization of training\n",
    "\t\tif (0 < self.video < 3): \n",
    "\t\t\tsimdata = [self.Nn, self.trial, self.latency]\n",
    "\t\t\tactdata = [self.x_position, self.y_position, self.x_position_old, self.y_position_old]\n",
    "\t\t\tRdata = self.reward_position\n",
    "\t\t\tSdata = [self.x_start, self.y_start]\n",
    "\t\t\tQdata = self.Q[self.x_position][self.y_position][:]\n",
    "\t\t\tself.sarsaad.visualization(self.video, simdata, actdata, Rdata, Sdata, Qdata)\n",
    "\t\t# visualization of latency\n",
    "\t\tif self.video == 3:\n",
    "\t\t\tif (self.trial % int(self.N_trials / 25) == 0 or self.trial == self.N_trials - 1):    \n",
    "\t\t\t\tself.sarsaad.latency(self.latency_list,self.N_trials,self.Nn)\n",
    "\t\t\t\ttime.sleep(0.5)\n",
    "        \n",
    "        \n",
    "\t\"\"\"=======================================================================================================\"\"\"\n",
    "\t\"\"\"                                                TO DO                                                  \"\"\"\n",
    "\t\"\"\"=======================================================================================================\"\"\"\n",
    "\n",
    "\n",
    "\t################################################\n",
    "\t\"\"\"             SARSA algorithm              \"\"\"\n",
    "\t################################################\n",
    "        \n",
    "\tdef run_trial(self):\n",
    "\t\t# run the trial\n",
    "\t\tself.init_trial()\n",
    "\t\twhile True:\n",
    "\t\t\tself.run_episode()\n",
    "\t\t\t# visualize learning process\n",
    "\t\t\tif (0 < self.video < 3): self.visualization()\n",
    "\t\t\t# check if agent position is a goal\n",
    "\t\t\tif self.is_trial_end(): break\n",
    "\t\treturn self.latency\n",
    "    \n",
    "    \n",
    "    \n",
    "\t################################################\n",
    "\t\"\"\"        Initialization  functions         \"\"\"\n",
    "\t################################################\n",
    "    \n",
    "    \n",
    "\tdef init_parameters(self): \n",
    "\t\t# number of trials\n",
    "\t\tself.N_trials = 10000\n",
    "\t\tself.epsilon = 1.0\n",
    "\t\tself.tau = self.N_trials/4\n",
    "\t\tself.eta = 0.3\n",
    "\t\tself.gamma = 0.8\n",
    "\t\tself.lambda_eligibility = 0.6\n",
    "        \n",
    "\t\t# initialize the Q-values and the eligibility trace\n",
    "\t\tself.Q = numpy.zeros((self.Nn,self.Nn,4))\n",
    "\t\tself.e = numpy.zeros((self.Nn,self.Nn,4))\n",
    "        \n",
    "\n",
    "\tdef init_trial(self):\n",
    "\t\tself.e = numpy.zeros((self.Nn,self.Nn,4))\n",
    "\t\tself.epsilon = np.exp(-self.trial/(self.tau/1.0))\n",
    "        \n",
    "\t\t# choose the initial position\n",
    "\t\twhile True:\n",
    "\t\t\tself.x_start = numpy.random.randint(0,self.Nn)\n",
    "\t\t\tself.y_start = numpy.random.randint(0,self.Nn)\n",
    "\t\t\tself.x_position = self.x_start; self.y_position = self.y_start\n",
    "\t\t\tif not self.is_block(): break\n",
    "\n",
    "\t\tself.choose_action()\n",
    "\t\tself.latency = 0.0\n",
    "\n",
    "        \n",
    "\tdef run_episode(self):\n",
    "\t\tself.update_state()\n",
    "\t\tself.choose_action()  \n",
    "\t\tself.update_E()\n",
    "\t\tself.update_Q()\n",
    "\t\tself.latency += 1\n",
    "\n",
    "    \n",
    "\t#########################################\n",
    "\t\"\"\"        Updates functions      \"\"\"\n",
    "\t#########################################\n",
    "    \n",
    "\tdef update_state(self):\n",
    "\t\t# remember the old position of the agent\n",
    "\t\tself.x_position_old = self.x_position\n",
    "\t\tself.y_position_old = self.y_position\n",
    "        \n",
    "\t\t# update the agents position according to the action\n",
    "\t\t# move down\n",
    "\t\tif self.action == 0: self.x_position += 1\n",
    "\t\t# move up\n",
    "\t\telif self.action == 1: self.x_position -= 1\n",
    "\t\t# move right\n",
    "\t\telif self.action == 2: self.y_position += 1\n",
    "\t\t# move left\n",
    "\t\telif self.action == 3: self.y_position -= 1\n",
    "        \n",
    "    \n",
    "\tdef update_E(self):\n",
    "\t\t# update the eligibility trace\n",
    "\t\tself.e = self.lambda_eligibility * self.e\n",
    "\t\tself.e[self.x_position_old, self.y_position_old,self.action_old] += 1.\n",
    "    \n",
    "    \n",
    "\tdef update_Q(self):\n",
    "\t\t# update the Q-values\n",
    "\t\tself.Q +=  self.eta * self.e * \\\n",
    "\t\t\t          (self.reward() - \\\n",
    "\t\t\t          (self.Q[self.x_position_old,self.y_position_old,self.action_old] - \\\n",
    "\t\t\t           self.gamma * self.Q[self.x_position, self.y_position, self.action]))\n",
    "\n",
    "\n",
    "\t#############################################\n",
    "\t\"\"\"        Interaction functions        \"\"\"\n",
    "\t#############################################\n",
    "        \n",
    "\tdef choose_action(self):\n",
    "\t\t# choose the next action\n",
    "\t\tself.action_old = self.action\n",
    "\t\tif numpy.random.rand() < self.epsilon:\n",
    "\t\t\tind = numpy.random.randint(len(self.Actions[self.x_position][self.y_position]))\n",
    "\t\t\tself.action = self.Actions[self.x_position][self.y_position][ind]\n",
    "\t\telse:\n",
    "\t\t\tind = argmax(self.Q[self.x_position,self.y_position,self.Actions[self.x_position][self.y_position]])\n",
    "\t\t\tself.action = self.Actions[self.x_position][self.y_position][ind]\n",
    "    \n",
    "    \n",
    "\tdef reward(self):\n",
    "\t\t# Look at next positions, is it reward (goal)?\n",
    "\t\t# move down\n",
    "\t\tif self.action_old == 0: self.x_pos = self.x_position_old + 1; self.y_pos = self.y_position_old\n",
    "\t\t# move up\n",
    "\t\telif self.action_old == 1: self.x_pos = self.x_position_old - 1; self.y_pos = self.y_position_old\n",
    "\t\t# move right\n",
    "\t\telif self.action_old == 2: self.y_pos = self.y_position_old + 1; self.x_pos = self.x_position_old\n",
    "\t\t# move left\n",
    "\t\telif self.action_old == 3: self.y_pos = self.y_position_old - 1; self.x_pos = self.x_position_old\n",
    "            \n",
    "\t\t# return an actual reward\n",
    "\t\tif (self.x_pos == self.reward_position[0]) and (self.y_pos == self.reward_position[1]):\n",
    "\t\t\treturn 1.0\n",
    "\t\telse: return 0.0\n",
    "\n",
    "       \n",
    "\t#########################################\n",
    "\t\"\"\"       Checkup functions          \"\"\"\n",
    "\t#########################################\n",
    "        \n",
    "\tdef is_trial_end(self):\n",
    "\t\treturn (self.reward() == 1.0) or (self.latency >= self.Nn * self.Nn)\n",
    "\n",
    "\n",
    "\tdef is_block(self):\n",
    "\t\t# if action is impossible\n",
    "\t\tif len(self.Actions[self.x_position][self.y_position]) == 0.0:\n",
    "\t\t\treturn True\n",
    "\t\telse: return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "### C.3. Upload SARSA function into the storage <a id='C3'></a>\n",
    "Once you are done and ready to submit your solution, please, follow the next steps: </br>\n",
    "\n",
    "- Move to [C.2. SARSA function](#C2) and uncomment the first line, i.e. remove the `#` symbol before `%%writefile SARSA_solution.py` ;\n",
    "- Run cell [C.2. SARSA function](#C2) then you sholud see `Writing SARSA_solution.py` just above; \n",
    "- Run cell [C.3. Upload SARSA function into the storage](#C3) to save this file in your Collab storage (check your Collab storage to see if the file is there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_collab_storage(filepath='SARSA_solution.py', filetype='text/x-python', remove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "---------------------------\n",
    "## D. SARSA simulation <a id='D'></a>\n",
    "In this part, you can test your implementation of **C.2. SARSA function (TO DO)** using different modes of visualization. You can also save the results of Q-value calculations into a file and upload this file to your Collab storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### D.1. Perform SARSA training (editable) <a id='D1'></a>\n",
    "The cell below run [your script](#C2) so as to simulate SARSA training. You can visualize this process.\n",
    "- *sarsa = SARSA(visualization)* # upload your SARSA function [**visualization**: No - **0**, Mode-1 - **1**, Mode-2 - **2**, Latency - **3**];</br> \n",
    "- *sarsa.run_sarsa()* # run simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sarsa = SARSA(3)\n",
    "sarsa.run_sarsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.2. Save SARSA training result <a id='D2'></a>\n",
    "The result of the SARSA training is a *navigation heatmap* based on Q-values. This navigation heatmap is represented by a table after the training simulation. The Q-values have been saved into the file __*'SARSA_data_Qvalue.csv'*__ which is now contained in this Jupyter notebook. The next command will upload the __*'SARSA_data_Qvalue.csv'*__ to your Collab storage. \n",
    "Run the cell below and check your Collab storage to see if the __*'SARSA_data_Qvalue.csv'*__ is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_collab_storage(filepath='SARSA_data_Qvalue.csv', filetype='text/csv', remove=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "\n",
    "-------------------------------\n",
    "## E. SARSA Evaluation <a id='E'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the SARSA training is to obtain a navigation map based on Q-values. Such a map should provide the shortest path between any position of the robot environment and any goal position. The following command collects information about robot movements using this *Q-navigation map*. By running this comand, you will get the statistics of all robot paths to the goal. You can visualize this process and check how the robot performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from epflx_robox_nrp_utils.SARSA.SARSA_evaluation import SARSA_evaluation\n",
    "sarsaev = SARSA_evaluation()\n",
    "# Run evaluation with a specific mode\n",
    "from IPython.display import clear_output\n",
    "try: video = input(\"To visualize the evaluation process, please, input '1': \")\n",
    "except: video = 0;\n",
    "clear_output()\n",
    "\n",
    "# Analysis\n",
    "best, good, over, never = sarsaev.run_evaluation(video)\n",
    "# Statistics\n",
    "print 'Agent came to the goal in', best+good, 'of', best+good+never,'cases.'\n",
    "print 'Agent came to the goal by the shortest way in', best,'cases.'\n",
    "print 'Agent came to the goal by non-shortest way in', good,'cases using', over, 'steps over.'\n",
    "print 'Agent did not come to the goal in', never, 'cases.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------\n",
    "\n",
    "-----------------------------------------------\n",
    "## F. Robot navigation within NRP platform (SOM & SARSA application) <a id='F'></a>\n",
    "The goal of this last section is to reuse the SARSA training results in a robotics simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.1. Generate test for NRP platform <a id='F1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from epflx_robox_nrp_utils.SARSA.SARSA_evaluation import SARSA_evaluation\n",
    "sarsaev = SARSA_evaluation()\n",
    "sarsaev.test_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.2. Save NRP test <a id='F2'></a>\n",
    "We have just generated a path for the robot navigation within the NRP platform. \n",
    "This path is written into the file __*'SARSA_data_way_points.csv'*__ which is now contained in this Jupyter notebook space and consists of [x,y] coordinates of the positions the robot will go through. The following command upload the __*'SARSA_data_way_points.csv'*__ into your Collab storage. \n",
    "\n",
    "Run the cell below and check your Collab storage to see if the __*'SARSA_data_way_points.csv'*__ is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_collab_storage(filepath='SARSA_data_way_points.csv', filetype='text/csv', remove=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.3. Run experiment within NRP platform <a id='F3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the results obtained with the SOM and SARSA algorithms. Thanks to these results, we will navigate a simulated robot within a virtual environment of the [Neurorobotics platform](http://148.187.97.48/#/esv-private?dev). To launch the relevant simulation, please, follow these instructions below.\n",
    "\n",
    "- Download the files __'SOM_lattice_data.csv'__ and __'SARSA_data_way_points.csv'__ from your Collab storage.\n",
    "- Join the [Neurorobotics platform](http://148.187.97.48/#/esv-private?dev) and select the \"Templates\" tab.\n",
    "- Select *Exercise_3* in the list of Templates (use the search filter) and press the *Clone* button.\n",
    "- Select the \"Experiment files\" tab and open the *Exercise_3* folder.\n",
    "- Upload the files __'SOM_lattice_data.csv'__ and __'SARSA_data_way_points.csv'__ into the Experiment files storage.\n",
    "- Move back to the \"My experiments\" tab and launch *Exercise_3*.\n",
    "- Run the simulation and check if the robot navigates through the specfified waypoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Submission and grading\n",
    "Run the next cell to submit your SARSA implementation. Each submission is automatically tested and evaluated. Only the last submission will be taken into account for your grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = get_hbp_service_client()\n",
    "collab_path = get_collab_storage_path()\n",
    "print(\"Retrieving your HBP OIDC token ...\")\n",
    "token = str(oauth.get_token())\n",
    "print(\"token retrieved!\")\n",
    "submission_info = {\n",
    "    'subheader': 'Exercise 3',\n",
    "    'filepath': 'SARSA_function.py',\n",
    "    'token': token,\n",
    "    'collab_path': collab_path,\n",
    "    'clients_storage': clients.storage\n",
    "}\n",
    "from epflx_robox_nrp_utils.submission_manager.submission_widget import display_submission_widget\n",
    "display_submission_widget(submission_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
