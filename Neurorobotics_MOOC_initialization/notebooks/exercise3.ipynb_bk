{"cells":[{"metadata":{},"cell_type":"markdown","source":["# Exercise 3\n","# Navigate the robot between any two points of its environment"]},{"metadata":{},"cell_type":"markdown","source":["--------------------------------------\n","\n","--------------------------------------\n","### Initialization. Get data from solved Excercise 1\n","\n","*Explanation how to go over from Excercise 2 to Excercise 3*"]},{"metadata":{},"cell_type":"markdown","source":["--------------------------------------------\n","\n","--------------------------------------------\n","\n","### Content:\n","- <font size=\"4\">**[A. Utils installation and data downloading](#A)**</font></br>\n",">- <font size=\"3\">[A.1. Install SOM utils](#A1)</font></br>\n",">- <font size=\"3\">[A.2. Download from this Collab's Storage](#A2)</font></br>\n","- <font size=\"4\">**[B. Perform SOM training (editable) ](#B)**</font></br>\n","- <font size=\"4\">**[C. Complete the SARSA implementation](#C)**</font></br>\n",">- <font size=\"3\">[C.1. SARSA description](#C1)</font></br>\n",">- <font size=\"3\">[C.2. SARSA function (TO DO)](#C2)</font></br>\n",">- <font size=\"3\">[C.3. Upload SARSA function into the storage](#C3)</font></br>\n","- <font size=\"4\">**[D. SARSA simulation ](#D)**</font></br>\n",">- <font size=\"3\">[D.1. Perform SARSA training (editable) ](#D1)</font></br>\n",">- <font size=\"3\">[D.2. Save SOM training result ](#D2)</font></br>\n","- <font size=\"4\">**[E. Result evaluation](#E)**</font></br>\n","- <font size=\"4\">**[F. Robot navigation within NRP platform (SOM & SARSA application)](#F)**</font></br>\n",">- <font size=\"3\">[F.1. Generate test for NRP platform ](#F1)</font></br>\n",">- <font size=\"3\">[F.2. Save NRP test ](#F2)</font></br>\n",">- <font size=\"3\">[F.3. Run experiment within NRP platform](#F3)</font></br>"]},{"metadata":{},"cell_type":"markdown","source":["---------------------------------------------------\n","\n","---------------------------------------------------\n","## A. Utils installation and data downloading<a id='A'></a>"]},{"metadata":{},"cell_type":"markdown","source":["### A.1. Install SOM utils <a id='A1'></a>\n","sdfsdsfdf"]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":["from IPython.display import clear_output\n","! pip uninstall epflx_robox_nrp_utils -y\n","! pip install --user --force-reinstall git+https://github.com/HBPNeurorobotics/EPFLx-RoboX-Neurorobotics-utils#egg=epflx_robox_nrp_utils\n","! pip install --upgrade \"hbp-service-client==1.1.1\"\n","clear_output()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### A.2. Download from this Collab's Storage <a id='A2'></a>\n","Download the default robot_positions.csv file from this Collab's Storage"]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["clients = get_hbp_service_client()\n","collab_path = get_collab_storage_path()\n","\n","filename = raw_input(\"Enter name of data-file your want to use for simulation:\")\n","if(filename==''): filename = 'robot_positions.csv'; clear_output()\n","clients.storage.download_file(collab_path + '/' + filename, './robot_positions.csv')\n","\n","filename = raw_input(\"Enter name of data-file your want to use for simulation:\")\n","if(filename==''): filename = 'SOM_data_lattice.csv'; clear_output()\n","clients.storage.download_file(collab_path + '/' + filename, './SOM_data_lattice.csv')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["-----------------------------------------------------\n","\n","-----------------------------------------------------\n","## B. SARSA data preparation: generate your maze based on the SOM data <a id='B'></a>\n","In the Exercise 2 we have got the SOM-lattice that basically plays role of data reduction. At this step we make transition from space of Cartesian coordinates of environment to SOM-lattice coordinates. It means the robot will moves straigt from one point of SOM-lattice to another. But not all of them are available to come. So, here we define an availability of directions to go and generate reward matrix based on this information. Thus, you will go through next step here:\n","\n","- You can see 2 images wich represents the translation from Cartesian coordinates to SOM-lattice representation of space.\n","- Input the goal position that you want to use as [y,x] in SOM-lattice coordinates.\n","- Analyse retrived Reward matrix that has next trusture [N,N,4], where **N** is size of SOM-lattice and **4** is number of possible actions (Down, Up, Right, Left). "]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["from epflx_robox_nrp_utils.SARSA.SARSA_additional import SARSA_additional\n","sarsaad = SARSA_additional()\n","reward, actions = sarsaad.run_processing()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# if you create new SOM-function\n","import os; filename = 'SOM_possible_actions.csv'\n","if(os.path.isfile(filename)):\n","    # if old version of SARSA_data_Qvalue.csv exists\n","    if(clients.storage.exists(collab_path+'/'+filename)): \n","        clients.storage.delete(collab_path+'/'+filename)\n","    pydata = clients.storage.upload_file(filename, collab_path + '/'+filename, 'text/csv')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["-----------------------------------------------------\n","\n","-----------------------------------------------------\n","## C. SARSA data preparation: generate your maze based on the SOM data <a id='C'></a>"]},{"metadata":{},"cell_type":"markdown","source":["### C.1. SARSA description <a id='C1'></a>\n","****\n","\n","**Exercise goal:**<br>\n","Your task is to implement a state-action-reward-state-action (SARSA) algorithm that explores the environment where the agent evolves.\n","\n","**Input:** SOM-lattice **given by Exercise 2** ( <font color=red>*'SOM_data_lattice.csv'*</font> ).<br>\n","Running the <font color=green>**_SARSA data preparation_**</font> cell you have to generate **a maze** and **a reward table** ( <font color=red>*'SARSA_data_reward.csv'*</font> ) based on SOM-lattice data.<br>\n","\n","**Output:** Expected reward matrix [Q-values] ( <font color=red>*'SARSA_data_Qvalue.csv'*</font> ).<br>\n","Running the <font color=green>**_NRP platform test_**</font> cell you can generate a random **start position** (or enter yours) and **way points** ( <font color=red>*'SARSA_data_way_points.csv'*</font> ) to reach trained goal based on your SARSA learning.<br>\n","****\n","\n","**SARSA algorithm:**<br>\n","*<font color=green>**while**</font> (N_trials are not done):*<br>\n","> <font color=blue>*# TRIAL: run_trial()*</font><br>\n","> *initialize eligibility*<br>\n","> *update policy (greedy epsilon)*<br>\n","> *generate random start position (take care the availability of that position)*<br>\n","> *make a choice of action: choose_action()*<br>\n","> *<font color=green>**while**</font> (agent is not in goal):*<br>\n",">> *<font color=blue># EPISODE: run_episode():</font>*<br>\n",">> *change current agent position: update_state()*<br>\n",">> *make a choice of action: choose_action()*<br>\n",">> *update the eligibility trace: update_E()*<br>\n",">> *update the expected reward: update_Q()*<br>\n",">> *update the latency parameter*<br>\n",">> *<font color=blue># ----------------------------------</font>*<br>\n",">> *checkup of trial end: agent in the goal (also, lattency limit can make trial end)*<br>\n","\n","****\n","\n","\n","**Functions of initialization:**<br>\n"," - **init_parameters():** genetrate new order of robot position points; <font color=green>**return**</font> *__array__ of new order*.<br>\n"," - **init_trial():** genetrate new order of robot position points; <font color=green>**return**</font> *__array__ of new order*.<br>\n"," \n","**Functions of updates:**<br>\n"," - **update_state():** Remember the old position of the agent before to move on. Update the agents position according to the chosen action. Check if the agent has bumped into a wall (move to is_wall).; <font color=green>**return**</font> *__nothing__*.<br>\n"," - **update_E():** Update the eligibility trace according to SARSA; <font color=green>**return**</font> *__nothing__*.<br>\n"," - **update_Q():** Update the Q-values according to SARSA; <font color=green>**return**</font> *__nothing__*.<br>\n"," \n","**Functions of interaction:**<br>\n"," - **choose_action():** Choose the next action based on the current estimate of the Q-values. The parameter epsilon determines, how often agent chooses the action with the highest Q-value (probability 1-epsilon). In the rest of the cases a random action is chosen; <font color=green>**return**</font> *__nothing__*.<br>\n"," - **reward():** Evaluates how much reward should be administered when performing the chosen action at the current location; <font color=green>**return**</font> *__reward__ value for chosen action*.<br>\n","\n","**Functions of checkup:**<br>\n"," - **is_trial_end():** checkup is it the trial end; <font color=green>**return**</font> *boolean value*.<br>\n"," - **is_bump():** checkup does agent bump into the wall; <font color=green>**return**</font> *boolean value*.<br>\n"," - **is_wall():** chenkup does agent set up on the wall; <font color=green>**return**</font> *boolean value*.<br>\n","****\n","\n","\n","**Global variables initialization (<font color=blue>default</font>, <font color=red>by students</font>):**<br>\n"," - <font color=blue>**self.video:**</font> the visualization trigger (0 - no; 1 - mode1; 2 - mode2; 3 - latency).<br>\n"," - <font color=blue>**self.pos:**</font> the data from input file ( <font color=red>*'NRP_data_robot_positions.csv'*</font> ).<br>\n"," - <font color=blue>**self.lattice:**</font> the initial SOM randomly generated.<br>\n"," - <font color=blue>**self.Nn:**</font> the parameter of Maze size that equals to SOM size [_Nn_ x _Nn_].<br>\n"," <br>\n"," - <font color=blue>**self.reward_position:**</font> the goal position. You can enter yours running the <font color=green>**_SARSA data preparation_**</font> cell, otherwise the goal position will be generated randomly.<br>\n"," - <font color=blue>**self.Reward:**</font> the reward table taken from input file ( <font color=red>*'SARSA_data_reward.csv'*</font> ) that is generated by running of the <font color=green>**_SARSA data preparation_**</font> cell.<br>\n"," <br>\n"," - <font color=blue>**self.latency:**</font> the latency of current trial (initial value is 0).<br>\n"," - <font color=blue>**self.latency_list:**</font> is **empty list** by default. This list will accumulate latency of each trial.<br>\n"," - <font color=blue>**self.trial:**</font> the index of current trial (initial value is 0).<br>\n","<br>\n"," - <font color=blue>**[self.x_position, self.y_position]:**</font> the current position of agent (in terms of SARSA algorithm - **_S'_**).<br>\n"," - <font color=blue>**[self.x_position_old, self.y_position_old]:**</font> the previous position of agent (in terms of SARSA algorithm - **_S_**).<br>\n"," - <font color=blue>**[self.x_start, self.y_start]:**</font> the random position of agent at the very begining of trial.<br>\n"," - <font color=blue>**self.action:**</font> the current chosen action of agent (in terms of SARSA algorithm - **_A'_**).<br>\n"," - <font color=blue>**self.action_old:**</font> the previous chosen action of agent (in terms of SARSA algorithm - **_A_**).<br>\n","<br>\n"," - <font color=red>**self.N_trials:**</font> the number of trials during learning.<br>\n"," - <font color=red>**self.Q:**</font> the **initial** state of the expected reward matrix [*__Nn__* x *__Nn__* x __4__] (represents the 4 possible actions: **up, down, right, left**). <br>\n"," - <font color=red>**self.e:**</font> the **initial** stete of the eligibility trace matrix [*__Nn__* x *__Nn__* x __4__].<br>\n"," <br>\n"," - <font color=red>**self.reward_at_target:**</font> the reward when agent has reached the goal.<br>\n"," - <font color=red>**self.reward_at_wall:**</font> the reward (punishment) when agent has bumped to the wall.<br>\n","<br>\n"," - <font color=red>**self.epsilon:**</font> the **initial** probability at which the agent chooses a random action. This makes sure the agent explores the maze (**Policy: epsilon greedy**).<br>\n"," - <font color=red>**self.tau:**</font>  the time constant of SARSA algorithm.<br>\n"," - <font color=red>**self.eta:**</font> the learning rate of SARSA algorithm.<br>\n"," - <font color=red>**self.gamma:**</font> the discount factor - quantifies how far into the future a reward is still considered important for the current action.<br>\n"," - <font color=red>**self.lambda_eligibility:**</font> the the decay factor for the eligibility trace the default is 0., which corresponds to no eligibility trace at all.<br>\n","****\n","\n","<font color=red>**TO DO:**</font><br>\n","**1)** Run the <font color=green>**_SARSA data preparation_**</font> cell. You can see the representations (mode1, mode2) of the maze generated based on your SOM-lattice solution. Please, enter the goal coordinates to get reward table. Then, press enter to finish this stage.<br>\n","**2)** Fill the hole (<font color=red>**episode**</font>) in SARSA algorithm **run_trial()**.<br>\n","**3)** Fill <font color=red>the Functions of initialization</font> of parameters <font color=red>(by students)</font> for the very beggining learning **init_parameters()** and needed parameters for the start of each trial **init_trial()**.<br>\n","**4)** Implement the <font color=red>Functions of updates</font>: agent position, eligibility trace and expected reward.<br>\n","**5)** Implement the <font color=red>Functions of interactons</font>: choose action (baced on the policy) and get reward for this action.<br>\n","**6)** Implement the <font color=red>Functions of checkup</font>: make agent check itself [is_trial_end(); is_bump(); is_wall()]<br>\n","**7)** Run the <font color=green>**_NRP platform test_**</font> cell to generate a random **start position** (or enter yours) and **way points** ( <font color=red>*'SARSA_data_way_points.csv'*</font> ) to reach trained goal based on your SARSA learning.<br>\n","**6)** Run <font color=green>**_NRP platform_**</font> and open the <font color=green>__*Miniproject2_solution*__</font>. Upload the two csv-files (<font color=red>*'SOM_data_lattice.csv'*</font> , <font color=red>*'SARSA_data_way_points.csv'*</font>). Then, **run the project to see result**. <br>\n","****\n","\n","<font color=red>**IMPORTANT:**</font><br>\n","**1)** The first line of SARSA implementation template: <font color=green>*%%writefile __SARSA_Surname_Name.py__*</font>.<br> Please, replace _'Surname'_ and _'Name'_ with your own Surname and Name. Don't uncomment this line until you are ready to submit.<br>\n","**2)** Please, pay attention that the grading process has the timelimit of your function simulation. **The timelimit** is equal to **60 seconds**.<br>\n","**3)** Once you are ready to submit, please, uncomment the first line of script ( <font color=green>*%%writefile __SARSA_Surname_Name.py__*</font> ) and run cell. Thus, your script will be saved as file with such name. **Then, submit this file.**\n","\n","---------------------------------------\n","\n","---------------------------------------"]},{"metadata":{},"cell_type":"markdown","source":["### C.2. SARSA function (TO DO) <a id='C2'></a>"]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["#%%writefile SARSA_solution.py \n","\n","# class SARSA (Sarsa_solution)                                                                                               \n","import numpy\n","import time\n","from pylab import *\n","import matplotlib.pyplot as plt\n","\n","\n","class SARSA:\n","\n","\t#########################################\n","\t###        Main stages of SARSA       ###\n","\t#########################################\n","    \n","\tdef __init__(self, video=0):\n","\t\t# upload lattice information\n","\t\tfrom epflx_robox_nrp_utils.SARSA.SARSA_additional import SARSA_additional\n","\t\tself.sarsaad = SARSA_additional()\n","\n","\t\tself.video = video\n","\t\tself.pos = self.sarsaad.upload_positions()\n","\t\tself.lattice = self.sarsaad.upload_lattice()\n","\t\tself.Nn = len(self.lattice[0])\n","        \n","\t\t# initialize parameters\n","\t\tself.init_parameters()\n","        \n","\t\t# reward administered the target location and when bumping into walls\n","\t\tself.Actions, self.reward_position = self.sarsaad.upload_reward()#(self)Reward\n","        \n","\t\t# list that contains the times it took the agent to reach the target for all trials\n","\t\t# serves to track the progress of learning\n","\t\tself.latency_list = []\n","\n","\t\t# initialize the state and action variables\n","\t\tself.x_position = None\n","\t\tself.y_position = None\n","\t\tself.action = None\n","\t\tself.x_position_old = None\n","\t\tself.y_position_old = None\n","\t\tself.action_old = None\n","        \n","\n","\tdef run_sarsa(self):\n","\t\tT = time.time()\n","\t\tfor self.trial in range(self.N_trials):\n","\t\t\t# run a trial and store the time it takes to the target\n","\t\t\tself.latency = self.run_trial()\n","            \n","\t\t\t# Add latency of last trial and visualize latency \n","\t\t\tif(self.trial%1000==0 or self.trial==self.N_trials): self.sarsaad.save_Qvalue(self.Q)\n","\t\t\tself.latency_list.append(self.latency)\n","\t\t\tif(self.video == 3): self.visualization()\n","                \n","\t\tself.sarsaad.print_Qvalue(self.Q, self.reward_position,self.Actions)\n","\t\tprint 'Done. Simulation time is ', time.time()-T, '(s).'\n","\n","        \n","\t############################################\n","\t###        Visualization of SARSA        ###\n","\t############################################\n","    \n","\tdef visualization(self):\n","\t\t# visualization of training\n","\t\tif(0 < self.video < 3): \n","\t\t\tsimdata = [self.Nn, self.trial, self.latency]\n","\t\t\tactdata = [self.x_position, self.y_position, self.x_position_old, self.y_position_old]\n","\t\t\tRdata = self.reward_position\n","\t\t\tSdata = [self.x_start, self.y_start]\n","\t\t\tQdata = self.Q[self.x_position][self.y_position][:]\n","\t\t\tself.sarsaad.visualization(self.video, simdata, actdata, Rdata, Sdata, Qdata)\n","\t\t# visualization of latency\n","\t\tif(self.video == 3):\n","\t\t\tif(self.trial%int(self.N_trials/25)==0 or self.trial==self.N_trials-1):    \n","\t\t\t\tself.sarsaad.latency(self.latency_list,self.N_trials,self.Nn)\n","\t\t\t\ttime.sleep(0.5)\n","        \n","        \n","\t\"\"\"=======================================================================================================\"\"\"\n","\t\"\"\"                                                TO DO                                                  \"\"\"\n","\t\"\"\"=======================================================================================================\"\"\"\n","\n","\n","\t################################################\n","\t\"\"\"             SARSA algorithm              \"\"\"\n","\t################################################\n","        \n","\tdef run_trial(self):\n","\t\t# run the trial\n","\t\tself.init_trial()\n","\t\twhile True:\n","\t\t\tself.run_episode()\n","\t\t\t# visualize learning process\n","\t\t\tif(0 < self.video < 3): self.visualization()\n","\t\t\t# check if agent position is a goal\n","\t\t\tif self.is_trial_end(): break\n","\t\treturn self.latency\n","    \n","    \n","    \n","\t################################################\n","\t\"\"\"        Functions of initialization       \"\"\"\n","\t################################################\n","    \n","    \n","\tdef init_parameters(self): \n","\t\t# number of trials\n","\t\tself.N_trials = 10000\n","\t\tself.epsilon = 1.0\n","\t\tself.tau = self.N_trials/4\n","\t\tself.eta = 0.3\n","\t\tself.gamma = 0.8\n","\t\tself.lambda_eligibility = 0.6\n","        \n","\t\t# initialize the Q-values and the eligibility trace\n","\t\tself.Q = numpy.zeros((self.Nn,self.Nn,4))\n","\t\tself.e = numpy.zeros((self.Nn,self.Nn,4))\n","        \n","\n","\tdef init_trial(self):\n","\t\tself.e = numpy.zeros((self.Nn,self.Nn,4))\n","\t\tself.epsilon = np.exp(-self.trial/(self.tau/1.0))\n","        \n","\t\t# choose the initial position\n","\t\twhile True:\n","\t\t\tself.x_start = numpy.random.randint(0,self.Nn)\n","\t\t\tself.y_start = numpy.random.randint(0,self.Nn)\n","\t\t\tself.x_position = self.x_start; self.y_position = self.y_start\n","\t\t\tif not self.is_block(): break\n","\n","\t\tself.choose_action()\n","\t\tself.latency = 0.0\n","\n","        \n","\tdef run_episode(self):\n","\t\tself.update_state()\n","\t\tself.choose_action()  \n","\t\tself.update_E()\n","\t\tself.update_Q()\n","\t\tself.latency += 1\n","\n","    \n","\t#########################################\n","\t\"\"\"        Functions of updates       \"\"\"\n","\t#########################################\n","    \n","\tdef update_state(self):\n","\t\t# remember the old position of the agent\n","\t\tself.x_position_old = self.x_position\n","\t\tself.y_position_old = self.y_position\n","        \n","\t\t# update the agents position according to the action\n","\t\t# move down\n","\t\tif   self.action == 0: self.x_position += 1\n","\t\t# move up\n","\t\telif self.action == 1: self.x_position -= 1\n","\t\t# move right\n","\t\telif self.action == 2: self.y_position += 1\n","\t\t# move left\n","\t\telif self.action == 3: self.y_position -= 1\n","        \n","    \n","\tdef update_E(self):\n","\t\t# update the eligibility trace\n","\t\tself.e = self.lambda_eligibility * self.e\n","\t\tself.e[self.x_position_old, self.y_position_old,self.action_old] += 1.\n","    \n","    \n","\tdef update_Q(self):\n","\t\t# update the Q-values\n","\t\tself.Q +=  self.eta * self.e * \\\n","\t\t\t          (self.reward() - \\\n","\t\t\t          (self.Q[self.x_position_old,self.y_position_old,self.action_old] - \\\n","\t\t\t           self.gamma * self.Q[self.x_position, self.y_position, self.action]))\n","\n","\n","\t#############################################\n","\t\"\"\"        Functions of interaction       \"\"\"\n","\t#############################################\n","        \n","\tdef choose_action(self):\n","\t\t# choose the next action\n","\t\tself.action_old = self.action\n","\t\tif numpy.random.rand() < self.epsilon:\n","\t\t\tind = numpy.random.randint(len(self.Actions[self.x_position][self.y_position]))\n","\t\t\tself.action = self.Actions[self.x_position][self.y_position][ind]\n","\t\telse:\n","\t\t\tind = argmax(self.Q[self.x_position,self.y_position,self.Actions[self.x_position][self.y_position]])\n","\t\t\tself.action = self.Actions[self.x_position][self.y_position][ind]\n","    \n","    \n","\tdef reward(self):\n","\t\t# Look at next positions, is it reward (goal)?\n","\t\t# move down\n","\t\tif   self.action_old == 0: self.x_pos = self.x_position_old + 1; self.y_pos = self.y_position_old\n","\t\t# move up\n","\t\telif self.action_old == 1: self.x_pos = self.x_position_old - 1; self.y_pos = self.y_position_old\n","\t\t# move right\n","\t\telif self.action_old == 2: self.y_pos = self.y_position_old + 1; self.x_pos = self.x_position_old\n","\t\t# move left\n","\t\telif self.action_old == 3: self.y_pos = self.y_position_old - 1; self.x_pos = self.x_position_old\n","            \n","\t\t# return an actual reward\n","\t\tif(self.x_pos==self.reward_position[0])and(self.y_pos==self.reward_position[1]):\n","\t\t\treturn 1.0\n","\t\telse: return 0.0\n","\n","       \n","\t#########################################\n","\t\"\"\"        Functions of checkup       \"\"\"\n","\t#########################################\n","        \n","\tdef is_trial_end(self):\n","\t\treturn (self.reward() == 1.0) or (self.latency >= self.Nn*self.Nn)\n","\n","\n","\tdef is_block(self):\n","\t\t# if action is impossible\n","\t\tif(len(self.Actions[self.x_position][self.y_position]) == 0.0):\n","\t\t\treturn True\n","\t\telse: return False"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["****\n","### <font color=red> C.3. Upload SARSA function into the storage</font> <a id='C3'></a>\n","Once you are done and ready to submit your solution, please, follow next steps: </br>\n","\n","- Move to [C.2. SARSA function](#C2) and uncomment the first line <font color=red> __*%%writefile SARSA_solution.py*__</font> ;\n","- Run cell [C.2. SARSA function](#C2) then you sholud see <font color=red> __*Writing SARSA_solution.py*__</font> just above; \n","- Run cell [C.3. Upload SARSA function into the storage](#C3) to save this file in storage (check your storage to see the file there)."]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["import os; filename = 'SARSA_solution.py'\n","# if you create new SOM-function\n","if(os.path.isfile(filename)):\n","    # if old version of SOM function exists\n","    if(clients.storage.exists(collab_path+'/'+filename)): \n","        clients.storage.delete(collab_path+'/'+filename)\n","    pydata = clients.storage.upload_file(filename, collab_path + '/'+filename, 'text/x-python')\n","    # remove \n","    os.remove(filename)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["---------------------------\n","\n","---------------------------\n","## D. SARSA simulation <a id='D'></a>\n","In this part you can test your implementation of **C.2. SARSA function (TO DO)** using diffetent modes of visualization. Then, you can save the file with results of Q-value calculations into your Collab's Storage."]},{"metadata":{"trusted":false,"collapsed":true},"cell_type":"markdown","source":["### D.1. Perform SARSA training (editable) <a id='D1'></a>\n","This cell runs the simulation of SARSA training that uses a [your implemented script](#C2). You can visualize this process.\n","- *sarsa = SARSA(visualization)* # upload your SARSA function [**visualization**: No - **0**, Mode-1 - **1**, Mode-2 - **2**, Latency - **3**];</br> \n","- *sarsa.run_sarsa()* # run simulation."]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":["sarsa = SARSA(3)\n","sarsa.run_sarsa()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### D.2. Save SOM training result <a id='D2'></a>\n","Result of SARSA training is Navigation \"heatmap\" based on Q-values (represented as table after training simulation). The Q-value data was saved into the file __*'SARSA_data_Qvalue.csv'*__ which is contained in Ipython space now. This commads upload the __*'SARSA_data_Qvalue.csv'*__ into your Collab's storage. \n","\n","Run the cell and then check your storage out to see the __*'SARSA_data_Qvalue.csv'*__ there."]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["# if you create new SOM-function\n","import os; filename = 'SARSA_data_Qvalue.csv'\n","if(os.path.isfile(filename)):\n","    # if old version of SARSA_data_Qvalue.csv exists\n","    if(clients.storage.exists(collab_path+'/'+filename)): \n","        clients.storage.delete(collab_path+'/'+filename)\n","    pydata = clients.storage.upload_file(filename, collab_path + '/'+filename, 'text/csv')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["-------------------------------\n","\n","-------------------------------\n","## E. SARSA evaluation <a id='E'></a>"]},{"metadata":{},"cell_type":"markdown","source":["The goal of SARSA training is the robot forma a Navigation map based on Q-values. Such map should provide the shortest way from any point within experiment environment to the defined goal position. This commands collect information about robot movements using this \"Q-navigation map\". Running this comand you get the statistics of robot pathes to the goal. You can visualize this process to check out how robot moves."]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":["from epflx_robox_nrp_utils.SARSA.SARSA_evaluation import SARSA_evaluation\n","sarsaev = SARSA_evaluation()\n","# Run evaluation with specific mode\n","from IPython.display import clear_output\n","try: video = input(\"To visualize an evaluation prosess, please, input '1': \")\n","except: video = 0;\n","clear_output()\n","\n","# Analysis\n","best, good, over, never = sarsaev.run_evaluation(video)\n","# Statistics\n","print 'Agent came to the goal in', best+good, 'of', best+good+never,'cases.'\n","print 'Agent came to the goal by the shortest way in', best,'cases.'\n","print 'Agent came to the goal by non-shortest way in', good,'cases using', over, 'steps over.'\n","print 'Agent did not come to the goal in', never, 'cases.'"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["-----------------------------------------------\n","\n","-----------------------------------------------\n","## F. Robot navigation within NRP platform (SOM & SARSA application) <a id='F'></a>\n","The goal of this part is to apply training results to simulation of robot movement within virtual environment."]},{"metadata":{},"cell_type":"markdown","source":["### F.1. Generate test for NRP platform <a id='F1'></a>"]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["from epflx_robox_nrp_utils.SARSA.SARSA_evaluation import SARSA_evaluation\n","sarsaev = SARSA_evaluation()\n","sarsaev.test_generation()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### F.2. Save NRP test <a id='F2'></a>\n","We have just generated a path for robot within NRP platform. This path is written into the file __*'SARSA_data_way_points.csv'*__ which is contained in Ipython space now and consists of [x,y] coordinates of points robot will go through. This commads upload the __*'SARSA_data_way_points.csv'*__ into your Collab's storage. \n","\n","Run the cell and then check your storage out to see the __*'SARSA_data_way_points.csv'*__ there."]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["# if you create new SOM-function\n","import os; filename = 'SARSA_data_way_points.csv'\n","if(os.path.isfile(filename)):\n","    # if old version of SARSA_data_Qvalue.csv exists\n","    if(clients.storage.exists(collab_path+'/'+filename)): \n","        clients.storage.delete(collab_path+'/'+filename)\n","    pydata = clients.storage.upload_file(filename, collab_path + '/'+filename, 'text/csv')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### F.3. Run experiment within NRP platform <a id='F3'></a>"]},{"metadata":{},"cell_type":"markdown","source":["Here we use results retrived via implementation of SOM and SARSA algorithms to navigate robot within virtual environment at [Neurorobotics platform](http://148.187.97.48/#/esv-private?dev). To realize such simulation, please, follow next guide: \n","\n","- Download from Collab's Storage the data files __'SOM_lattice_data.csv'__ and __'SARSA_data_way_points.csv'__.\n","- Open [Neurorobotics platform](http://148.187.97.48/#/esv-private?dev).\n","- Find Exercise_3 in the list of Templates.\n","- Open the tab \"Experiment files\" and find the Exercise_3.\n","- Upload the data files __'SOM_lattice_data.csv'__ and __'SARSA_data_way_points.csv'__ into Experiment files storage.\n","- Move back to tab \"My experiments\" and launch Exercise_3.\n","- Run the simulation to see how robot will move navigated by Nevigation created by you."]},{"metadata":{},"cell_type":"markdown","source":["."]},{"metadata":{},"cell_type":"markdown","source":["."]},{"metadata":{},"cell_type":"markdown","source":["."]},{"metadata":{},"cell_type":"markdown","source":["## ============================  SARSA evaluation  ==========================="]},{"metadata":{"trusted":true},"cell_type":"code","source":["import os\n","os.listdir('.')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["os.remove('SOM_data_lattice.csv')\n","os.remove('robot_positions.csv')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["test = os.listdir('.')\n","\n","for item in test:\n","    if item.endswith(\".csv\"):\n","        os.remove(os.path.join('.', item))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],index=[4, 5, 6], columns=['A', 'B', 'C'])\n","df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["df.at[0, 2] = 100.0"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["type(df)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["reward"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["actions"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["line = ['0', '[0,2]', ' [0,2,3]', '[0,2,3]', '[0,2,3]', '[0,2,3]', '[0,2,3]', '[0,2,3]', '[0,2,3]', '[0,2,3]', '[0,2,3]', '[0,2,3]', '[0,3]']"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["N = 12"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["from ast import literal_eval\n","\n","Reward = []\n","for j in range(1,N+1):\n","    if(len(line[j])%2==0)and(len(line[j])>2):\n","        reward_position = j; line[j] = line[j].replace(\" [\", \"[\")\n","    cell =  line[j]\n","    cell =  literal_eval(cell)\n","    Reward.append(cell)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["Reward"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["reward = [[0, 2], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [1, 3], [], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 3], [], [0, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 3], [], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [], [0, 1, 2], [0, 1, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [1, 2], [1, 3], [0, 2], [0, 1, 2, 3], [0, 1, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 2, 3], [0, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 3]]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["goal = [2,4]\n","Nn = 12\n","import numpy as np"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["def perfect_map(Nn, goal, reward):\n","\tN = Nn*Nn-1\n","\tM = np.zeros((Nn,Nn))+100\n","\tM[goal[0],goal[1]] = 0\n","\tprint goal\n","\tprint reward\n","\tprint M\n","\tt = 0; L = 0\n","\twhile N > L:\n","\t\tt += 1\n","\t\tfor i in range(Nn):\n","\t\t\tfor j in range(Nn):\n","\t\t\t\t#print i,j, len(reward[i*Nn+j])\n","\t\t\t\tif(len(reward[i*Nn+j])==0.0 and M[i,j] == 100.0): L = L + 1; M[i,j] = 1000; print \"L+1\"\n","\t\t\t\tif(M[i,j] < 100): \n","\t\t\t\t\tfor d in range(len(reward[i*Nn+j])):\n","\t\t\t\t\t\tif(reward[i*Nn+j][d] == 0):\n","\t\t\t\t\t\t\tif(M[i+1,j]>M[i,j]+1): \n","\t\t\t\t\t\t\t\t\t\t\tif(M[i+1,j]==100): N = N - 1\n","\t\t\t\t\t\t\t\t\t\t\tM[i+1,j] = M[i,j] + 1;\n","\t\t\t\t\t\tif(reward[i*Nn+j][d] == 1):\n","\t\t\t\t\t\t\tif(M[i-1,j]>M[i,j]+1): \n","\t\t\t\t\t\t\t\t\t\t\tif(M[i-1,j]==100): N = N - 1\n","\t\t\t\t\t\t\t\t\t\t\tM[i-1,j] = M[i,j] + 1;\n","\t\t\t\t\t\tif(reward[i*Nn+j][d] == 2):\n","\t\t\t\t\t\t\tif(M[i,j+1]>M[i,j]+1): \n","\t\t\t\t\t\t\t\t\t\t\tif(M[i,j+1]==100): N = N - 1\n","\t\t\t\t\t\t\t\t\t\t\tM[i,j+1] = M[i,j] + 1; \n","\t\t\t\t\t\tif(reward[i*Nn+j][d] == 3):\n","\t\t\t\t\t\t\tif(M[i,j-1]>M[i,j]+1): \n","\t\t\t\t\t\t\t\t\t\t\tif(M[i,j-1]==100): N = N - 1\n","\t\t\t\t\t\t\t\t\t\t\tM[i,j-1] = M[i,j] + 1; \n","\t\tprint N, L\n","\tM[goal[0],goal[1]] = 100\n","\tM = np.where(M==100, 0, M)\n","\tprint M\n","\treturn M"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["perfect_map(Nn, goal, reward)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["%%HTML\n","<style type=\"text/css\">\n","table.dataframe td, table.dataframe th {\n","    border: 0px, 1px black solid !important;\n","  color: black !important;\n","border-style: dashed none none none;\n","}\n","</style>"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.DataFrame(np.random.rand(5,2), columns=[\"a\",\"b\"])\n","df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["\"\"\"\n","Show examples of modifying the Excel output generated by pandas\n","\"\"\"\n","import pandas as pd\n","import numpy as np\n","\n","from xlsxwriter.utility import xl_rowcol_to_cell\n","\n","\n","df = pd.read_excel(\"../in/excel-comp-datav2.xlsx\")\n","\n","# We need the number of rows in order to place the totals\n","number_rows = len(df.index)\n","\n","# Add some summary data using the new assign functionality in pandas 0.16\n","df = df.assign(total=(df['Jan'] + df['Feb'] + df['Mar']))\n","df = df.assign(quota_pct=(1+(df['total'] - df['quota'])/df['quota']))\n","\n","# Create a Pandas Excel writer using XlsxWriter as the engine.\n","# Save the unformatted results\n","writer_orig = pd.ExcelWriter('simple.xlsx', engine='xlsxwriter')\n","df.to_excel(writer_orig, index=False, sheet_name='report')\n","writer_orig.save()\n","\n","# Create a Pandas Excel writer using XlsxWriter as the engine.\n","writer = pd.ExcelWriter('fancy.xlsx', engine='xlsxwriter')\n","df.to_excel(writer, index=False, sheet_name='report')\n","\n","# Get access to the workbook and sheet\n","workbook = writer.book\n","worksheet = writer.sheets['report']\n","\n","# Reduce the zoom a little\n","worksheet.set_zoom(90)\n","\n","# Add a number format for cells with money.\n","money_fmt = workbook.add_format({'num_format': '$#,##0', 'bold': True})\n","\n","# Add a percent format with 1 decimal point\n","percent_fmt = workbook.add_format({'num_format': '0.0%', 'bold': True})\n","\n","# Total formatting\n","total_fmt = workbook.add_format({'align': 'right', 'num_format': '$#,##0',\n","                                 'bold': True, 'bottom':6})\n","# Total percent format\n","total_percent_fmt = workbook.add_format({'align': 'right', 'num_format': '0.0%',\n","                                         'bold': True, 'bottom':6})\n","\n","# Format the columns by width and include number formats\n","\n","# Account info columns\n","worksheet.set_column('B:D', 20)\n","# State column\n","worksheet.set_column('E:E', 5)\n","# Post code\n","worksheet.set_column('F:F', 10)\n","\n","# Monthly columns\n","worksheet.set_column('G:K', 12, money_fmt)\n","# Quota percent columns\n","worksheet.set_column('L:L', 12, percent_fmt)\n","\n","# Add total rows\n","for column in range(6, 11):\n","    # Determine where we will place the formula\n","    cell_location = xl_rowcol_to_cell(number_rows+1, column)\n","    # Get the range to use for the sum formula\n","    start_range = xl_rowcol_to_cell(1, column)\n","    end_range = xl_rowcol_to_cell(number_rows, column)\n","    # Construct and write the formula\n","    formula = \"=SUM({:s}:{:s})\".format(start_range, end_range)\n","    worksheet.write_formula(cell_location, formula, total_fmt)\n","\n","# Add a total label\n","worksheet.write_string(number_rows+1, 5, \"Total\",total_fmt)\n","percent_formula = \"=1+(K{0}-G{0})/G{0}\".format(number_rows+2)\n","worksheet.write_formula(number_rows+1, 11, percent_formula, total_percent_fmt)\n","\n","# Define our range for the color formatting\n","color_range = \"L2:L{}\".format(number_rows+1)\n","\n","# Add a format. Light red fill with dark red text.\n","format1 = workbook.add_format({'bg_color': '#FFC7CE',\n","                               'font_color': '#9C0006'})\n","\n","# Add a format. Green fill with dark green text.\n","format2 = workbook.add_format({'bg_color': '#C6EFCE',\n","                               'font_color': '#006100'})\n","\n","# Highlight the top 5 values in Green\n","worksheet.conditional_format(color_range, {'type': 'top',\n","                                           'value': '5',\n","                                           'format': format2})\n","\n","# Highlight the bottom 5 values in Red\n","worksheet.conditional_format(color_range, {'type': 'bottom',\n","                                           'value': '5',\n","                                           'format': format1})\n","\n","writer.save()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["M = np.random.rand(5,2)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["M = np.round(M,3)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["M"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["M[2][0] += 123*10e-7 "],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["M[2][0]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["actions = [[[0, 2], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 2, 3], [0, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [], [1, 2], [1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 2, 3], [0, 3], [2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [], [1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 3], [], [1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 3], [], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3]], [[1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 3]]]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["heatmap = [[ 0.,0.12548913,0.15409701,0.,0.,0.,0.,0.55263376,0.,0.87793422,0.,0.]\n"," [ 0.,0.,0.21615834,0.26329565,0.33257887,0.42078,0.,0.68281615,0.87485151,1.11546489,0.79859123,0.69813111]\n"," [ 0.17737554,0.22189394,0.28591076,0.35888424,0.44902702,0.53465619,0.68513506,0.89643216,1.12867405,1.39496875,1.09978456,0.85678289]\n"," [ 0.,0.,0.21950949,0.28129722,0.35447828,0.44486537,0.,0.,1.36617507,1.6873869,0.,0.]\n"," [ 0.17695576,0.2263092,0.28471252,0.35681676,0.44962384,0.,0.,0.,2.76034017,2.2080172,1.65122645,1.39729601]\n"," [ 0.,0.27323725,0.34355612,0.43038283,0.55115545,0.70468839,0.88787539,1.11689241,1.26492135,1.76467121,1.39707859,1.11478466]\n"," [ 0.,0.22644009,0.2874246,0.35800256,0.44607402,0.55538119,0.69153759,0.86171115,1.046331,1.40962181,1.1273884,0.88371779]\n"," [ 0.,0.17919234,0.2244191,0.28873393,0.,0.,0.55612422,0.70211053,0.87156504,1.1244826,0.90133763,0.        ]\n"," [ 0.,0.14276359,0.,0.23031885,0.,0.,0.,0.,0.68938642,0.88963043,0.72005227,0.56139096]\n"," [ 0.,0.1123396,0.14390779,0.17966252,0.,0.,0.,0.43968995,0.56027602,0.69091042,0.57505468,0.        ]\n"," [ 0.06956026,0.08866953,0.,0.14005427,0.17021954,0.22435284,0.28043275,0.3424551,0.43516231,0.52482183,0.44151688,0.        ]\n"," [ 0.,0.,0.09024611,0.11278682,0.14317043,0.17882238,0.22403232,0.,0.,0.,0.33476567,0.        ]]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["act = actions[0][1]\n","num = 0\n","for i in range(len(act)):\n","    num = num + (act[i]+1)*10**i\n","    \n","heat = 0.27323725\n","heat = np.round(heat,4)\n","heat = heat + num*10**(-(4+len(act)))\n","\n","cell = (heat - np.round(heat,4))*10**4"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["act[:] += 1"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["x = 0.432000001\n","np.round(x,5)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print act\n","print num\n","print heat\n","print cell"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["\t\toutheat = np.zeros((Q.shape[0],Q.shape[0]), dtype=float)\n","\t\tfor i in range(Q.shape[0]):\n","\t\t\tfor j in range(Q.shape[0]):\n","\t\t\t\tact = actions[i][j]\n","\t\t\t\tprint act\n","\t\t\t\tnum = 0\n","\t\t\t\tfor i in range(len(act)):\n","\t\t\t\t    num = num + (act[i]+1)*10**i\n","\t\t\t\t\n","\t\t\t\theat = heatmap[i][j]\n","\t\t\t\tprint heat\n","\t\t\t\theat = np.round(heat,5)\n","\t\t\t\theat = heat + num*10**(-(5+len(act)))\n","\t\t\t\toutheat[i,j] = heat\n","\t\t\t\tprint \"OUT\", outheat[i,j]\n","\t\t\n","\t\tprint outheat, outheat[2,1]\n","\t\t#combine = np.dstack((heatmap2,actions))\n","\t\t#print combine.shape, combine[0][1][1]"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python2","display_name":"Python 2","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython2","version":"2.7.14","file_extension":".py","codemirror_mode":{"version":2,"name":"ipython"}}},"nbformat":4,"nbformat_minor":2}