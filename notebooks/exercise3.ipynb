{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Navigate the robot between any two points of its environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA data preparation: generate your maze based on your SOM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse / SARSA data preparation (Verify possible links if both points are on the wall!!!!!!!!!!!!!!)\n",
    "#reward_position = [8,3] # choose goal position by yourself or generate as random position\n",
    "\n",
    "from SARSA_additional import SARSA_additional\n",
    "sarsaad = SARSA_additional()\n",
    "sarsaad.run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO ===> SARSA implementation function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Description, general comments, so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile SARSA_Surname_Name.py \n",
    "# class SARSA (Sarsa_solution)\n",
    "                                                                                               \n",
    "import numpy\n",
    "import time\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SARSA:\n",
    "    \"\"\"\n",
    "    A class that implements a quadratic NxN gridworld. \n",
    "    \n",
    "    Methods:\n",
    "    \n",
    "    learn(N_trials=100):\tRun 'N_trials' trials. \n",
    "\t\t\t\t\t\t\tA trial is finished, when the agent reaches the reward location.\n",
    "    visualize_trial()  : \tRun a single trial with graphical output.\n",
    "    reset()            : \tMake the agent forget everything he has learned.\n",
    "    plot_Q()           : \tPlot of the Q-values .\n",
    "    learning_curve()   : \tPlot the time it takes the agent to reach the target \n",
    "\t\t\t\t\t\t\tas a function of trial number. \n",
    "    navigation_map()   : \tPlot the movement direction with the highest Q-value for all positions.\n",
    "    \"\"\"    \n",
    "        \n",
    "    def __init__(self):\n",
    "        from SARSA_additional import SARSA_additional\n",
    "        self.sarsaad = SARSA_additional()\n",
    "        \"\"\"\n",
    "        Creates a quadratic NxN gridworld. \n",
    "\n",
    "        Mandatory argument:\n",
    "        N: size of the gridworld\n",
    "\n",
    "        Optional arguments:\n",
    "        reward_position = (x_coordinate,y_coordinate): the reward location\n",
    "        obstacle = True:  Add a wall to the gridworld.\n",
    "        \n",
    "        reward_position,obstacle=False, lambda_eligibility=0.2\n",
    "        \"\"\"    \n",
    "        \n",
    "        # gridworld size\n",
    "        self.pos = self.sarsaad.upload_positions()#(self)pos\n",
    "        self.lattice = self.sarsaad.upload_lattice()#(self)lattice\n",
    "        self.Nn = len(self.lattice[0])\n",
    "        self.Reward, self.reward_position = self.sarsaad.upload_reward()#(self)Reward\n",
    "        \n",
    "        \n",
    "        self.centers = {}\n",
    "        self.edges = {}\n",
    "        self.video = {}\n",
    "              \n",
    "\n",
    "        # reward administered t the target location and when\n",
    "        # bumping into walls\n",
    "        self.reward_at_target = 1.\n",
    "        self.reward_at_wall   = -0.5\n",
    "        \n",
    "        self.Trial = 0\n",
    "        self.Run = 0\n",
    "        self.N_trials = 3000\n",
    "        \n",
    "\n",
    "        # probability at which the agent chooses a random\n",
    "        # action. This makes sure the agent explores the grid.\n",
    "        self.epsilon = 0.9\n",
    "                                                                                                  \n",
    "        # learning rate\n",
    "        self.eta = 0.9\n",
    "\n",
    "        # discount factor - quantifies how far into the future\n",
    "        # a reward is still considered important for the\n",
    "        # current action\n",
    "        self.gamma = 0.5\n",
    "\n",
    "        # the decay factor for the eligibility trace the\n",
    "        # default is 0., which corresponds to no eligibility\n",
    "        # trace at all.\n",
    "        self.lambda_eligibility = 0.2\n",
    "    \n",
    "        # is there an obstacle in the room?\n",
    "        #self.obstacle = obstacle\n",
    "\n",
    "        # initialize the Q-values etc.\n",
    "        self._init_run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run(self,video):\n",
    "        #print \"run\"\n",
    "        self.video = video\n",
    "        self.latencies = zeros(self.N_trials)\n",
    "        self.tau = self.N_trials/4\n",
    "        \n",
    "        #for run in range(N_runs):\n",
    "        self._init_run()\n",
    "        #call reset() to reset Q-values and latencies, ie forget all he learnt \n",
    "        #self.reset()\n",
    "        latencies = self._learn_run()\n",
    "        self.latencies += latencies/self.N_trials\n",
    "        \n",
    "        \n",
    "        #self.getQvalue()\n",
    "        self.sarsaad.save_Qvalue(self.Q)\n",
    "        self.sarsaad.print_Qvalue(self.Q)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    # The remainder of methods is for internal use and only relevant to those of you\n",
    "    # that are interested in the implementation details\n",
    "    ###############################################################################################\n",
    "        \n",
    "    \n",
    "    def _init_run(self):\n",
    "        #print \"_init_run\"\n",
    "        \n",
    "        #Initialize the Q-values, eligibility trace, position etc.\n",
    "        \n",
    "        # initialize the Q-values and the eligibility trace\n",
    "        self.Q = 0.01 * numpy.random.rand(self.Nn,self.Nn,4) + 0.1\n",
    "        self.e = numpy.zeros((self.Nn,self.Nn,4))\n",
    "        \n",
    "        # list that contains the times it took the agent to reach the target for all trials\n",
    "        # serves to track the progress of learning\n",
    "        self.latency_list = []\n",
    "\n",
    "        # initialize the state and action variables\n",
    "        self.x_position = None\n",
    "        self.y_position = None\n",
    "        self.action = None\n",
    "    \n",
    "    def _learn_run(self):\n",
    "        #print \"_learn_run\"\n",
    "        \n",
    "        #Run a learning period consisting of N_trials trials. \n",
    "        \n",
    "        #Options:\n",
    "        #N_trials :     Number of trials\n",
    "\n",
    "        #Note: The Q-values are not reset. Therefore, running this routine\n",
    "        #several times will continue the learning process. If you want to run\n",
    "        #a completely new simulation, call reset() before running it.\n",
    "        \n",
    "        \n",
    "        for self.trial in range(self.N_trials):            \n",
    "            self.Trial = self.trial\n",
    "            self.Run = 0\n",
    "            self.e = numpy.zeros((self.Nn,self.Nn,4))\n",
    "            self.epsilon = np.exp(-self.Trial/1.0*self.tau)\n",
    "            # run a trial and store the time it takes to the target\n",
    "            latency = self._run_trial()\n",
    "            if(self._reward() == 1): self.latency_list.append(latency)\n",
    "            else: self.latency_list.append(3*self.Nn*self.Nn)\n",
    "            if(self.video == 3): \n",
    "                if(self.trial%int(self.N_trials/25)==0 or self.trial==self.N_trials-1):    \n",
    "                    self.sarsaad.latency(self.latency_list,self.N_trials,self.Nn)\n",
    "                    if(self.trial==self.N_trials-1): time.sleep(15)\n",
    "                    else: time.sleep(0.5)\n",
    "\n",
    "        return array(self.latency_list)\n",
    "    \n",
    "    \n",
    "    def _run_trial(self,visualize=False):\n",
    "        #print \"_run_trial\"\n",
    "        \n",
    "        #Run a single trial on the gridworld until the agent reaches the reward position.\n",
    "        #Return the time it takes to get there.\n",
    "\n",
    "        #Options:\n",
    "        #visual: If 'visualize' is 'True', show the time course of the trial graphically\n",
    "        \n",
    "        # choose the initial position and make sure that its not in the wall\n",
    "        while True:\n",
    "            self.x_position = numpy.random.randint(self.Nn)\n",
    "            self.y_position = numpy.random.randint(self.Nn)\n",
    "            \n",
    "            self.x_start = self.x_position\n",
    "            self.y_start = self.y_position\n",
    "            \n",
    "            if not self._is_wall(self.x_position,self.y_position):\n",
    "                break\n",
    "        \n",
    "        \n",
    "        #print \"Starting trial at position ({0},{1}), reward at ({2},{3})\".format(self.x_position,self.y_position,self.reward_position[0],self.reward_position[1])\n",
    "        #if self.obstacle:\n",
    "        #      print \"Obstacle is in position (?,?)\"\n",
    "\n",
    "        # initialize the latency (time to reach the target) for this trial\n",
    "        latency = 0.\n",
    "\n",
    "        # start the visualization, if asked for\n",
    "        if visualize:\n",
    "            self._init_visualization()    \n",
    "            \n",
    "        # run the trial\n",
    "        #print self.Trial\n",
    "        self._choose_action()\n",
    "        arrived = False\n",
    "        while (not arrived) and (latency < 3*self.Nn*self.Nn): #(not self._arrived()) and (not arrived):\n",
    "            self._update_state()\n",
    "            self._choose_action()    \n",
    "            arrived = self._update_Q()\n",
    "            self.Run += 1\n",
    "            if visualize:\n",
    "                self._visualize_current_state()\n",
    "        \n",
    "            latency = latency + 1\n",
    "\n",
    "        if visualize:\n",
    "            self._close_visualization()\n",
    "        return latency\n",
    "    \n",
    "    ###############################################################################################\n",
    "    # \n",
    "    #\n",
    "    ###############################################################################################\n",
    "\n",
    "    def _update_Q(self):\n",
    "        #print \"_update_Q\"\n",
    "        \"\"\"\n",
    "        Update the current estimate of the Q-values according to SARSA.\n",
    "        \"\"\"\n",
    "        # update the eligibility trace\n",
    "        self.e = self.lambda_eligibility * self.e\n",
    "        self.e[self.x_position_old, self.y_position_old,self.action_old] += 1.\n",
    "        \n",
    "        # update the Q-values\n",
    "        if self.action_old != None:\n",
    "            self.Q +=     \\\n",
    "                self.eta * self.e *\\\n",
    "                (self._reward()  \\\n",
    "                - ( self.Q[self.x_position_old,self.y_position_old,self.action_old] \\\n",
    "                - self.gamma * self.Q[self.x_position, self.y_position, self.action] )  )\n",
    "        \n",
    "        if(self._reward() != 0.0): return True\n",
    "        else: return False\n",
    "\n",
    "    def _choose_action(self):  \n",
    "        #print \"_choose_action\"\n",
    "        \"\"\"\n",
    "        Choose the next action based on the current estimate of the Q-values.\n",
    "        The parameter epsilon determines, how often agent chooses the action \n",
    "        with the highest Q-value (probability 1-epsilon). In the rest of the cases\n",
    "        a random action is chosen.\n",
    "        \"\"\"\n",
    "        self.action_old = self.action\n",
    "        if numpy.random.rand() < self.epsilon:\n",
    "            self.action = numpy.random.randint(4)\n",
    "        else:\n",
    "            self.action = argmax(self.Q[self.x_position,self.y_position,:])    \n",
    "    \n",
    "    def _arrived(self):\n",
    "        #print \"_arrived\"\n",
    "        \"\"\"\n",
    "        Check if the agent has arrived.\n",
    "        \"\"\"\n",
    "        return ((self.x_position == self.reward_position[0] and \\\n",
    "\t\t\t\tself.y_position == self.reward_position[1])) or \\\n",
    "\t\t\t\t(self.Run > 3*self.Nn*self.Nn)# or \\\n",
    "\t\t\t\t#(self.Reward[self.x_position, self.y_position,self.action] != 0.0)\n",
    "\n",
    "    def _reward(self):\n",
    "        #print \"_reward\"\n",
    "        \"\"\"\n",
    "        Evaluates how much reward should be administered when performing the \n",
    "        chosen action at the current location\n",
    "        \"\"\"\n",
    "        return self.Reward[self.x_position_old, self.y_position_old,self.action_old]\n",
    "\n",
    "    def _update_state(self):\n",
    "        #print \"_update_state\"\n",
    "        \"\"\"\n",
    "        Update the state according to the old state and the current action.    \n",
    "        \"\"\"\n",
    "        # remember the old position of the agent\n",
    "        self.x_position_old = self.x_position\n",
    "        self.y_position_old = self.y_position\n",
    "        \n",
    "        # update the agents position according to the action\n",
    "        #  move right\n",
    "        if self.action == 0:\n",
    "            self.x_position += 1\n",
    "        # move left\n",
    "        elif self.action == 1:\n",
    "            self.x_position -= 1\n",
    "        # move up\n",
    "        elif self.action == 2:\n",
    "            self.y_position += 1\n",
    "        # move down\n",
    "        elif self.action == 3:\n",
    "            self.y_position -= 1\n",
    "        else:\n",
    "            print \"There must be a bug. This is not a valid action!\"\n",
    "                        \n",
    "        # check if the agent has bumped into a wall.\n",
    "        if self._is_wall():\n",
    "            self.x_position = self.x_position_old\n",
    "            self.y_position = self.y_position_old\n",
    "            self._wall_touch = True\n",
    "            #print \"#### wally ####\"\n",
    "        else:\n",
    "            self._wall_touch = False\n",
    "        \n",
    "        #if(self.video == 1): self.visualization1()\n",
    "        #if(self.video == 2): self.visualization2()\n",
    "        #from SARSA_additional import SARSA_additional\n",
    "        #sarsaad = SARSA_additional()\n",
    "        \n",
    "        if(0 < self.video < 3): \n",
    "\t\t\tsimdata = [self.Nn, self.Trial, self.Run]\n",
    "\t\t\tactdata = [self.x_position, self.y_position, self.x_position_old, self.y_position_old]\n",
    "\t\t\tRdata = self.reward_position\n",
    "\t\t\tSdata = [self.x_start, self.y_start]\n",
    "\t\t\tQdata = self.Q[self.x_position][self.y_position][:]\n",
    "\t\t\tself.sarsaad.visualization(self.video, simdata, actdata, Rdata, Sdata, Qdata)\n",
    "\t\t\tif(self._wall_touch): time.sleep(3)\n",
    "\n",
    "        \n",
    "\n",
    "    def _is_wall(self,x_position=None,y_position=None):    \n",
    "        #print \"_is_wall\"\n",
    "        \"\"\"\n",
    "        This function returns, if the given position is within an obstacle\n",
    "        If you want to put the obstacle somewhere else, this is what you have \n",
    "        to modify. The default is a wall that starts in the middle of the room\n",
    "        and ends at the right wall.\n",
    "\n",
    "        If no position is given, the current position of the agent is evaluated.\n",
    "        \"\"\"\n",
    "        if x_position == None or y_position == None:\n",
    "            x_position = self.x_position\n",
    "            y_position = self.y_position\n",
    "        \n",
    "        # check of the agent is trying to leave the gridworld\n",
    "        if x_position < 0 or x_position >= self.Nn or y_position < 0 or y_position >= self.Nn:\n",
    "            return True\n",
    "        \"\"\"\n",
    "        # check if the agent has bumped into an obstacle in the room\n",
    "        if self.obstacle:\n",
    "            if y_position == self.Nn/2 and x_position>self.Nn/2:\n",
    "                return True\n",
    "        \"\"\"\n",
    "        # if action is possible\n",
    "        if(max(self.Reward[self.x_position,self.y_position,:]) < 0.0):\n",
    "            return True\n",
    "        \n",
    "        # if none of the above is the case, this position is not a wall\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulation (SARSA training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q structure:\n",
      "Up:\n",
      "['  0.02987036', '  0.26666667', '  0.53333333', '  0.26666667', '  0.13333333', '  0.06666667', '  0.03333333', '  0.01666667', '  0.00833333', '  0.00416667', '  0.00208333', '  0.00104167']\n",
      "['  0.10054420', '  0.53333333', '  1.06666667', '  0.10873143', '  0.02961240', '  0.04629772', '  0.01705251', '  0.02007964', '  0.01318247', '  0.00703268', '  0.00202459', '  0.00174632']\n",
      "['  0.04781097', '  0.10640883', '  0.10240816', '  0.10356724', '  0.04446072', '  0.03187425', '  0.02090480', '  0.00832663', '  0.00658804', '  0.00336735', '  0.00093341', '  0.00056619']\n",
      "[' -0.04018444', '  0.10105745', '  0.10137107', '  0.04654496', '  0.04529686', '  0.02364884', '  0.01646456', ' -0.04157742', ' -0.04922527', '  0.00082008', '  0.00038277', ' -0.04029511']\n",
      "['  0.10030142', '  0.10355595', '  0.10118619', '  0.04760520', '  0.04444586', '  0.02124163', '  0.01106258', '  0.01175408', ' -0.39490877', '  0.00050768', '  0.00020973', '  0.00011905']\n",
      "['  0.03485441', '  0.03615391', '  0.10223837', '  0.03794846', '  0.01821320', '  0.01131217', '  0.00682021', '  0.00273205', '  0.10241761', ' -0.04099763', '  0.00008656', ' -0.04264625']\n",
      "['  0.01792757', '  0.00762268', '  0.03650862', ' -0.05186023', '  0.01265995', '  0.01066802', '  0.00580522', '  0.00362359', '  0.00068894', ' -0.41989358', '  0.00013021', ' -0.04119899']\n",
      "['  0.01340170', '  0.00569943', '  0.01737626', ' -0.39289843', ' -0.03957482', '  0.00624453', '  0.00353783', '  0.00084790', '  0.00036419', '  0.00038204', '  0.00019773', '  0.00010359']\n",
      "['  0.00429226', '  0.00748816', '  0.01253020', '  0.10520455', ' -0.41832805', '  0.00363040', '  0.00193017', '  0.00047802', '  0.00016181', '  0.00010748', '  0.00007666', '  0.00002680']\n",
      "['  0.00185485', '  0.00298198', '  0.00957431', '  0.00352651', '  0.00085569', '  0.00116013', '  0.00074578', '  0.00023164', '  0.00012902', '  0.00011014', '  0.00002356', ' -0.04216496']\n",
      "['  0.00106045', '  0.00234195', '  0.00395730', ' -0.04724627', ' -0.04037897', ' -0.04886661', '  0.00031789', '  0.00021260', ' -0.04819045', ' -0.04950174', ' -0.04115128', ' -0.05045087']\n",
      "[' -0.39050617', ' -0.38999283', ' -0.39407268', ' -0.39170333', ' -0.39019950', ' -0.39434416', ' -0.39189261', ' -0.39025815', ' -0.39327825', ' -0.39402378', ' -0.39095301', ' -0.39481788']\n",
      "\n",
      "Down:\n",
      "[' -0.39175812', '  0.10793967', ' -0.39018819', '  0.10545638', ' -0.39477323', ' -0.39057597', ' -0.39228740', ' -0.39461615', ' -0.39127854', ' -0.38974467', ' -0.39405045', ' -0.39454938']\n",
      "[' -0.04093730', '  0.10560057', '  0.10469489', '  0.10080722', '  0.04681624', '  0.04682638', '  0.02936694', '  0.02180265', ' -0.04099867', '  0.00532734', '  0.00192378', '  0.00202490']\n",
      "['  0.13333333', '  0.08360250', '  0.10569680', '  0.26666667', '  0.03675503', '  0.06666667', '  0.02407841', '  0.01666667', '  0.00833333', '  0.00411922', '  0.00174811', '  0.00104167']\n",
      "['  0.04907799', '  0.53333333', '  1.06666667', '  0.13333333', '  0.04004100', '  0.02349190', '  0.01666667', '  0.00748518', '  0.00412245', '  0.00208333', '  0.00104167', '  0.00052083']\n",
      "[' -0.04173186', '  0.26666667', '  0.53333333', '  0.04699234', '  0.02981099', '  0.02040546', '  0.01658209', '  0.00841179', '  0.00208333', '  0.00104167', '  0.00046786', ' -0.05070823']\n",
      "['  0.02844702', '  0.09143307', '  0.26666667', '  0.04692818', '  0.06666667', '  0.03333333', '  0.01321223', '  0.00833333', '  0.10029571', '  0.00052083', '  0.00026042', '  0.00013021']\n",
      "['  0.03333333', '  0.06666667', '  0.13333333', '  0.06666667', '  0.03333333', '  0.01666667', '  0.00833333', ' -0.04252443', ' -0.39367090', '  0.00026042', '  0.00009765', ' -0.03974719']\n",
      "[' -0.05225385', '  0.03333333', '  0.06666667', '  0.03333333', '  0.01666667', '  0.00707131', '  0.00381119', '  0.00206068', ' -0.05250273', ' -0.39015009', '  0.00019089', '  0.00010830']\n",
      "['  0.00776922', '  0.00698538', '  0.03333333', '  0.10269881', '  0.00833333', '  0.00416667', '  0.00208333', '  0.00103281', '  0.00046664', '  0.00026042', '  0.00012882', '  0.00006510']\n",
      "['  0.00416667', '  0.00831997', '  0.01666667', ' -0.39485265', ' -0.39204778', '  0.00208333', '  0.00104167', '  0.00051939', '  0.00020117', '  0.00012188', '  0.00006510', '  0.00003255']\n",
      "[' -0.04179379', '  0.00416667', '  0.00833333', '  0.00324312', '  0.00164741', '  0.00104167', '  0.00052083', '  0.00026042', '  0.00013021', '  0.00006510', '  0.00003222', '  0.00001628']\n",
      "['  0.00104167', '  0.00208333', '  0.00416667', '  0.00208333', '  0.00104163', '  0.00052083', '  0.00026042', '  0.00013021', '  0.00006510', '  0.00003255', '  0.00001628', '  0.00000814']\n",
      "\n",
      "Right:\n",
      "['  0.13333333', '  0.10472291', '  0.10001338', '  0.10288524', '  0.04227542', '  0.02713098', ' -0.04356187', '  0.00603842', '  0.00314064', '  0.00151507', ' -0.04922221', ' -0.39313980']\n",
      "['  0.26666667', '  0.10255369', '  0.10987862', '  0.10509907', '  0.03492496', '  0.04621797', '  0.04195941', '  0.02856984', '  0.01600745', '  0.00595696', '  0.00165082', ' -0.38995359']\n",
      "['  0.05495125', '  1.06666667', '  0.13333333', '  0.04490929', '  0.04657105', '  0.01975669', '  0.02105518', '  0.00615493', '  0.00364565', '  0.00178318', ' -0.03988587', ' -0.38973500']\n",
      "['  0.26666667', '  0.10076697', '  0.04638228', '  0.04594474', '  0.03941302', '  0.02046656', '  0.00760085', '  0.00651574', '  0.00328170', '  0.00177551', '  0.00043746', ' -0.39457742']\n",
      "['  0.13333333', '  0.10011046', '  0.10039279', '  0.04677987', '  0.04617964', '  0.02359886', '  0.01803147', ' -0.39266924', '  0.00117894', '  0.00054343', '  0.00024649', ' -0.39080565']\n",
      "['  0.06666667', '  0.13333333', '  0.04602106', '  0.04615963', '  0.01980449', '  0.01958115', '  0.00527308', ' -0.39289815', '  0.10768265', '  0.00041054', '  0.00015258', ' -0.38988689']\n",
      "['  0.01704112', ' -0.05345667', ' -0.42088462', '  0.01901389', '  0.01579235', '  0.00729135', '  0.00364185', ' -0.04156853', ' -0.39057999', '  0.00011013', '  0.00005755', ' -0.39295799']\n",
      "['  0.01666667', ' -0.04071187', ' -0.39046648', '  0.01560896', '  0.01112775', '  0.00389321', '  0.00190071', '  0.00158071', ' -0.04017143', '  0.00017085', '  0.00013705', ' -0.39195817']\n",
      "['  0.00833333', '  0.01666667', ' -0.39031054', '  0.10003842', '  0.00309600', '  0.00367494', '  0.00123735', '  0.00058594', '  0.00030863', '  0.00009365', ' -0.05138784', ' -0.39217876']\n",
      "['  0.00391119', '  0.00833333', '  0.00514741', '  0.00523826', '  0.00104167', '  0.00162367', '  0.00058908', '  0.00029574', '  0.00014477', '  0.00009610', ' -0.04172442', ' -0.39176743']\n",
      "['  0.00208333', '  0.00416667', '  0.00304289', '  0.00247043', '  0.00071087', '  0.00070070', '  0.00033845', '  0.00010393', '  0.00008461', '  0.00006182', '  0.00002025', ' -0.39293478']\n",
      "['  0.00104167', '  0.00208333', '  0.00294158', '  0.00150113', '  0.00040252', '  0.00051202', ' -0.04014766', '  0.00006155', '  0.00005128', '  0.00002420', ' -0.04277758', ' -0.39326984']\n",
      "\n",
      "Left:\n",
      "[' -0.39282135', '  0.05207020', '  0.10135088', ' -0.04000631', '  0.02937288', ' -0.04920606', ' -0.04023983', ' -0.04201890', '  0.00833333', '  0.00370528', ' -0.04036620', '  0.00104167']\n",
      "[' -0.39098082', '  0.10529781', '  0.10695432', '  0.53333333', '  0.26666667', '  0.13333333', '  0.06666667', '  0.03333333', '  0.01666667', '  0.00833333', '  0.00416667', '  0.00208333']\n",
      "['  0.10355060', '  0.10852552', '  0.10623650', '  0.10064046', '  0.13333333', '  0.03910677', '  0.03333333', '  0.01377466', '  0.00812169', '  0.00416667', '  0.00208333', '  0.00104167']\n",
      "[' -0.39215976', '  0.02905176', '  0.10132399', '  0.04693173', '  0.06666667', '  0.03333333', '  0.01643127', '  0.00833333', '  0.00416667', '  0.00206801', '  0.00104167', '  0.00052083']\n",
      "[' -0.39020321', '  0.10252417', '  0.06397104', '  0.26666667', '  0.13333333', '  0.06666667', '  0.03333333', '  0.01666667', ' -0.39124285', '  0.00103453', '  0.00052083', '  0.00026042']\n",
      "[' -0.39057526', ' -0.04013287', '  0.10513780', '  0.13333333', '  0.03831330', '  0.01943903', '  0.01666667', '  0.00765319', '  0.10910678', ' -0.39081481', '  0.00026013', '  0.00013021']\n",
      "[' -0.39338810', '  0.01906116', '  0.04787757', ' -0.39090542', ' -0.04750015', '  0.01066529', '  0.00785748', '  0.00416667', '  0.00208333', ' -0.39200657', ' -0.05314387', '  0.00006510']\n",
      "[' -0.39011013', ' -0.04016980', '  0.02889879', ' -0.39050408', ' -0.04060257', '  0.00833333', '  0.00416667', '  0.00208333', '  0.00104167', '  0.00052083', '  0.00026042', '  0.00013021']\n",
      "[' -0.39290808', ' -0.04204547', '  0.01260071', '  0.10075098', ' -0.38964956', ' -0.04697780', '  0.00206151', '  0.00104167', '  0.00052083', '  0.00026037', '  0.00013021', '  0.00006510']\n",
      "[' -0.39238108', '  0.00390343', '  0.00838952', '  0.00833333', ' -0.04921389', ' -0.04150255', '  0.00103426', '  0.00052083', '  0.00026042', '  0.00013021', '  0.00006510', '  0.00003255']\n",
      "[' -0.39253309', '  0.00163353', '  0.00554721', '  0.00416667', '  0.00208333', '  0.00096210', '  0.00052083', '  0.00023009', '  0.00013021', '  0.00006510', '  0.00003255', '  0.00001628']\n",
      "[' -0.39347719', '  0.00158502', ' -0.03995175', ' -0.04310314', '  0.00104167', '  0.00052083', '  0.00026042', ' -0.04195468', '  0.00006510', '  0.00003255', '  0.00001628', '  0.00000814']\n"
     ]
    }
   ],
   "source": [
    "#from SARSA_Surname_Name import SARSA\n",
    "sarsa = SARSA()\n",
    "sarsa.run(0); # sarsa.run(N_trials, Visualization[0-None, 1-Mode_1, 2-Mode_2, 3-Latency])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 30, 60.0, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SARSA_evaluation import SARSA_evaluation\n",
    "sarsaev = SARSA_evaluation()\n",
    "sarsaev.run_evaluation(0)  # run_evaluation(N_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test for NRP platform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps: 10\n",
      "[[10 10  9  8  7  6  5  4  3  2]\n",
      " [ 3  2  2  2  2  2  2  2  2  2]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAFrCAYAAAAaWNg/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a2a7f4b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from SARSA_evaluation import SARSA_evaluation\n",
    "sarsaev = SARSA_evaluation()\n",
    "sarsaev.test_generation() # test_generation(random or input yours [x,y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============================  SARSA evaluation  ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5]\n",
      "Function: 0.001 0.001 0.001 0.001\n",
      "Solution: 1 1 1 1\n",
      "Result:  0.00245 / 2.45 = 0.001 0.0\n",
      "[5, 5]\n",
      "Function: 0.001 0.001 0.001 0.001\n",
      "Solution: 1 1 1 1\n",
      "Result:  0.00245 / 2.45 = 0.001 0.0\n",
      "[5, 9]\n",
      "Function: 0.001 0.001 0.001 0.001\n",
      "Solution: 1 1 1 1\n",
      "Result:  0.00245 / 2.45 = 0.001 0.0\n"
     ]
    }
   ],
   "source": [
    "from SARSA_autograduation import SARSA_autograduation\n",
    "sarsaar = SARSA_autograduation()\n",
    "sarsaar.one_function_graduation('SARSA_Ihor_Kuras.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 4 0 0 137\n",
      "Solution: 83 58 116.0 0\n",
      "Result:  4.0 / 138.1 = 0.0289645184649 0.0\n"
     ]
    }
   ],
   "source": [
    "from SARSA_autograduation import SARSA_autograduation\n",
    "sarsaar = SARSA_autograduation()\n",
    "sarsaar.all_functions_graduation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SARSA_autograduation import SARSA_autograduation\n",
    "sarsaar = SARSA_autograduation()\n",
    "sarsaar.open_webpage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00  -1.87437218e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00  -6.46699719e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   2.00000000e+00   7.73118370e-03]\n",
      " ..., \n",
      " [  3.00000000e+00   9.00000000e+00   7.00000000e+00  -1.13990435e-03]\n",
      " [  3.00000000e+00   9.00000000e+00   8.00000000e+00  -1.44219763e-02]\n",
      " [  3.00000000e+00   9.00000000e+00   9.00000000e+00   3.16580397e-04]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   7.41845219e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00  -2.07863797e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   2.00000000e+00   9.74846040e-03]\n",
      " ..., \n",
      " [  3.00000000e+00   9.00000000e+00   7.00000000e+00   1.25328999e-03]\n",
      " [  3.00000000e+00   9.00000000e+00   8.00000000e+00   5.84754574e-04]\n",
      " [  3.00000000e+00   9.00000000e+00   9.00000000e+00  -1.20030047e-02]]\n",
      "2.04780844122\n"
     ]
    }
   ],
   "source": [
    "# upload a graduation standard from file\n",
    "import csv\n",
    "import math\n",
    "standard = np.zeros((4*10*10, 4))\n",
    "with open('SOLVED_data_Qvalue_sarsa.csv', 'r') as csvFile:\n",
    "\treader = csv.reader(csvFile)\n",
    "\tfor i,row in enumerate(reader):\n",
    "\t\tif(i>0): standard[i-1][:] = map(float,row)\n",
    "csvFile.close()\n",
    "print standard\n",
    "\n",
    "result = np.zeros((4*10*10, 4))\n",
    "with open('data_Qvalue_sarsa.csv', 'r') as csvFile:\n",
    "\treader = csv.reader(csvFile)\n",
    "\tfor i,row in enumerate(reader):\n",
    "\t\tif(i>0): result[i-1][:] = map(float,row)\n",
    "csvFile.close()\n",
    "print result\n",
    "\n",
    "diff = 0\n",
    "for i in range(4*10*10):\n",
    "    diff += (result[i][3]-standard[i][3])**2\n",
    "print math.sqrt(diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
